#!/usr/bin/env python3
"""
å¹¶è¡Œå•OPUSç”Ÿæˆè„šæœ¬ - æ”¯æŒå¤šè„šæœ¬*å¤šå¹¶å‘
åŸºäºgenerate_single_opus.pyæ”¹é€ 
"""

import asyncio
import argparse
import subprocess
import re
import sys
import random
import uuid
from pathlib import Path
from uuid import UUID

backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

import edge_tts
from sqlalchemy import text
from app.db.session import SessionLocal

def clean_markdown_for_tts(text: str) -> str:
    """æ¸…ç†markdownè¯­æ³•ï¼Œä¼˜åŒ–TTSæœ—è¯»"""
    if not text:
        return text
    
    if text.strip().startswith('```json'):
        try:
            import json
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(1))
                content_parts = []
                for item in json_data:
                    if isinstance(item, dict) and 'zh' in item:
                        content_parts.append(item['zh'])
                text = '\n\n'.join(content_parts)
        except:
            pass
    
    text = re.sub(r'```[^`]*```', '', text)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'#{1,6}\s*', '', text)
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

async def generate_single_tts(paper_id: str, content: str, voice: str, output_path: Path):
    """ç”Ÿæˆå•ä¸ªTTSæ–‡ä»¶"""
    try:
        # éšæœºå»¶è¿Ÿé¿å…APIé™åˆ¶
        await asyncio.sleep(random.uniform(0.5, 1.2))
        
        clean_content = clean_markdown_for_tts(content)
        
        if len(clean_content) < 10:
            print(f"âŒ å†…å®¹è¿‡çŸ­: {paper_id}")
            return False
        
        communicate = edge_tts.Communicate(clean_content, voice)
        
        # å”¯ä¸€ä¸´æ—¶æ–‡ä»¶åé¿å…å†²çª
        temp_wav = output_path.parent / f"temp_{uuid.uuid4().hex[:8]}.wav"
        
        await communicate.save(str(temp_wav))
        
        cmd = [
            'ffmpeg', '-i', str(temp_wav),
            '-c:a', 'libopus',
            '-b:a', '24k',
            '-vbr', 'on',
            '-compression_level', '10',
            '-frame_duration', '60',
            '-y', str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if temp_wav.exists():
            temp_wav.unlink()
        
        if result.returncode != 0:
            print(f"âŒ FFmpegè½¬æ¢å¤±è´¥: {paper_id}")
            return False
        
        if output_path.exists() and output_path.stat().st_size > 1000:
            file_size = output_path.stat().st_size / 1024
            print(f"âœ… {paper_id}: {file_size:.1f}KB")
            return True
        else:
            print(f"âŒ è¾“å‡ºæ–‡ä»¶æ— æ•ˆ: {paper_id}")
            return False
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥ {paper_id}: {e}")
        if 'temp_wav' in locals() and temp_wav.exists():
            temp_wav.unlink()
        return False

async def process_batch(papers, voice, output_dir, concurrency):
    """å¤„ç†ä¸€æ‰¹è®ºæ–‡"""
    semaphore = asyncio.Semaphore(concurrency)
    
    async def process_single(paper):
        async with semaphore:
            paper_id, title_en, title_zh, interpretation = paper
            
            output_path = output_dir / f"{paper_id}.opus"
            if output_path.exists() and output_path.stat().st_size > 1000:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {paper_id}")
                return True
            
            full_content = f"è®ºæ–‡æ ‡é¢˜ï¼š{title_zh}\nè‹±æ–‡æ ‡é¢˜ï¼š{title_en}\nAIè§£è¯»ï¼š{interpretation}"
            return await generate_single_tts(str(paper_id), full_content, voice, output_path)
    
    tasks = [process_single(paper) for paper in papers]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    success_count = sum(1 for r in results if r is True)
    return success_count, len(papers)

def get_papers_batch(db, offset, limit):
    """è·å–æŒ‡å®šèŒƒå›´çš„è®ºæ–‡"""
    query = text("""
        SELECT p.id, p.title_en, p.title_zh, p.ai_interpretation
        FROM papers p 
        WHERE p.ai_interpretation IS NOT NULL 
        AND LENGTH(p.ai_interpretation) > 100
        ORDER BY p.created_at DESC
        LIMIT :limit OFFSET :offset
    """)
    
    result = db.execute(query, {"limit": limit, "offset": offset})
    return result.fetchall()

async def main():
    parser = argparse.ArgumentParser(description="å¹¶è¡Œå•OPUSç”Ÿæˆ")
    parser.add_argument("--offset", type=int, default=0, help="èµ·å§‹åç§»é‡")
    parser.add_argument("--limit", type=int, default=50, help="å¤„ç†æ•°é‡")
    parser.add_argument("--concurrency", type=int, default=6, help="å¹¶å‘æ•°")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    parser.add_argument("--output-dir", default="./data/tts_single", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    db = SessionLocal()
    
    try:
        papers = get_papers_batch(db, args.offset, args.limit)
        
        if not papers:
            print("âŒ æœªæ‰¾åˆ°è®ºæ–‡")
            return
        
        print(f"ğŸµ å¤„ç† {len(papers)} ç¯‡è®ºæ–‡ (åç§»:{args.offset}, å¹¶å‘:{args.concurrency})")
        
        success, total = await process_batch(papers, args.voice, output_dir, args.concurrency)
        
        print(f"ğŸ‰ å®Œæˆ: {success}/{total} æˆåŠŸ")
        
    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(main())
