#!/usr/bin/env python3
"""
CSå€™é€‰æ± TTSç”Ÿæˆ - æ”¯æŒåˆ†ç‰‡å¹¶å‘å¤„ç†
åŸºäºproductionè„šæœ¬æ¶æ„ï¼Œæ”¯æŒoffset/limitå‚æ•°
"""
import asyncio
import argparse
import sys
import random
import uuid
import subprocess
import re
from pathlib import Path
from uuid import UUID
from datetime import datetime

backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

import edge_tts
import pendulum
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.db.session import SessionLocal
from app.services.data_ingestion.arxiv_candidate_pool import CandidatePoolServiceV2
from app.models.paper_tts import PaperTTS

def clean_markdown_for_tts(text: str) -> str:
    """æ¸…ç†markdownè¯­æ³•ï¼Œä¼˜åŒ–TTSæœ—è¯»"""
    if not text:
        return text
    
    if text.strip().startswith('```json'):
        try:
            import json
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(1))
                content_parts = []
                for item in json_data:
                    if isinstance(item, dict) and 'zh' in item:
                        content_parts.append(item['zh'])
                text = '\n\n'.join(content_parts)
        except:
            pass
    
    text = re.sub(r'```[^`]*```', '', text)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'#{1,6}\s*', '', text)
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    reraise=True
)
async def generate_single_tts(paper_id: str, content: str, voice: str, output_path: Path):
    """ç”Ÿæˆå•ä¸ªTTSæ–‡ä»¶ (å¸¦é‡è¯•æœºåˆ¶)"""
    try:
        await asyncio.sleep(random.uniform(0.1, 0.3))
        
        clean_content = clean_markdown_for_tts(content)
        
        if len(clean_content) < 10:
            print(f"âŒ å†…å®¹è¿‡çŸ­: {paper_id}")
            return False
        
        communicate = edge_tts.Communicate(clean_content, voice)
        temp_wav = output_path.parent / f"temp_{uuid.uuid4().hex[:8]}.wav"
        
        await communicate.save(str(temp_wav))
        
        cmd = [
            'ffmpeg', '-i', str(temp_wav),
            '-c:a', 'libopus',
            '-b:a', '24k',
            '-vbr', 'on',
            '-compression_level', '10',
            '-frame_duration', '60',
            '-y', str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if temp_wav.exists():
            temp_wav.unlink()
        
        if result.returncode != 0:
            print(f"âŒ FFmpegè½¬æ¢å¤±è´¥: {paper_id}")
            return False
        
        if output_path.exists() and output_path.stat().st_size > 1000:
            file_size = output_path.stat().st_size / 1024
            print(f"âœ… {paper_id}: {file_size:.1f}KB")
            return True
        else:
            print(f"âŒ è¾“å‡ºæ–‡ä»¶æ— æ•ˆ: {paper_id}")
            return False
            
    except Exception as e:
        print(f"âŒ TTSç”Ÿæˆå¤±è´¥ {paper_id}: {e}")
        raise  # Re-raise for tenacity to catch and retry

async def process_batch(papers, voice, output_dir, concurrency):
    """å¤„ç†ä¸€æ‰¹è®ºæ–‡"""
    semaphore = asyncio.Semaphore(concurrency)
    
    async def process_single(paper):
        async with semaphore:
            paper_id, title_en, title_zh, interpretation = paper
            
            output_path = output_dir / f"{paper_id}.opus"
            if output_path.exists() and output_path.stat().st_size > 1000:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {paper_id}")
                return paper_id, True
            
            full_content = f"è®ºæ–‡æ ‡é¢˜ï¼š{title_zh}\nè‹±æ–‡æ ‡é¢˜ï¼š{title_en}\nAIè§£è¯»ï¼š{interpretation}"
            success = await generate_single_tts(str(paper_id), full_content, voice, output_path)
            return paper_id, success
    
    tasks = [process_single(paper) for paper in papers]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return results

def get_cs_papers_batch(db, offset, limit, target_date):
    """è·å–CSå€™é€‰æ± è®ºæ–‡çš„æŒ‡å®šèŒƒå›´"""
    # è·å–CSå€™é€‰æ± æ‰€æœ‰è®ºæ–‡ID
    paper_ids = CandidatePoolServiceV2.get_candidate_papers_by_date(
        session=db,
        target_date=target_date,
        filter_type='cs'
    )
    
    # åˆ†ç‰‡å¤„ç†
    batch_ids = paper_ids[offset:offset + limit]
    
    if not batch_ids:
        return []
    
    # è·å–è®ºæ–‡å†…å®¹
    papers = []
    for paper_id in batch_ids:
        paper = db.execute(
            text("SELECT title FROM papers WHERE id = :paper_id"),
            {"paper_id": paper_id}
        ).fetchone()
        
        translation = db.execute(
            text("SELECT title_zh FROM paper_translations WHERE paper_id = :paper_id"),
            {"paper_id": paper_id}
        ).fetchone()
        
        interpretation = db.execute(
            text("SELECT interpretation FROM paper_interpretations WHERE paper_id = :paper_id"),
            {"paper_id": paper_id}
        ).fetchone()
        
        if paper and translation and interpretation:
            papers.append((paper_id, paper[0], translation[0], interpretation[0]))
    
    return papers

def save_tts_records(db, results, output_dir):
    """ä¿å­˜TTSè®°å½•åˆ°æ•°æ®åº“"""
    for paper_id, success in results:
        if success and isinstance(paper_id, UUID):
            output_path = output_dir / f"{paper_id}.opus"
            if output_path.exists():
                file_size = output_path.stat().st_size
                
                existing = db.query(PaperTTS).filter(PaperTTS.paper_id == paper_id).first()
                if not existing:
                    tts_record = PaperTTS(
                        paper_id=paper_id,
                        file_path=f"{paper_id}.opus",
                        file_size=file_size,
                        voice_model="zh-CN-XiaoxiaoNeural",
                        generated_at=datetime.utcnow()
                    )
                    db.add(tts_record)
    
    db.commit()

async def main():
    parser = argparse.ArgumentParser(description="CSå€™é€‰æ± TTSç”Ÿæˆ")
    parser.add_argument("--days-ago", type=int, help="å¤„ç†Nå¤©å‰çš„æ•°æ®")
    parser.add_argument("--date", type=str, help="æŒ‡å®šæ—¥æœŸ YYYY-MM-DD")
    parser.add_argument("--offset", type=int, default=0, help="èµ·å§‹åç§»é‡")
    parser.add_argument("--limit", type=int, default=50, help="å¤„ç†æ•°é‡")
    parser.add_argument("--concurrency", type=int, default=6, help="å¹¶å‘æ•°")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    parser.add_argument("--output-dir", default="/data/proj/flopap/data/tts_opus", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    # ç¡®å®šç›®æ ‡æ—¥æœŸ
    if args.date:
        target_date = pendulum.parse(args.date).date()
    elif args.days_ago is not None:
        target_date = (pendulum.today() - pendulum.duration(days=args.days_ago)).date()
    else:
        # é»˜è®¤é€»è¾‘: å¦‚æœæ²¡ä¼ ï¼Œå¯ä»¥æ ¹æ®å½“å‰ä¸šåŠ¡ä¹ æƒ¯é»˜è®¤ T-3
        target_date = (pendulum.today() - pendulum.duration(days=3)).date()

    print(f'ğŸš€ CSå€™é€‰æ± TTSç”Ÿæˆ - æ—¥æœŸ:{target_date} åç§»:{args.offset} æ•°é‡:{args.limit} å¹¶å‘:{args.concurrency}')
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    db = SessionLocal()
    
    try:
        # è·å–è®ºæ–‡æ‰¹æ¬¡
        papers = get_cs_papers_batch(db, args.offset, args.limit, target_date)
        print(f'ğŸ“ è·å–è®ºæ–‡: {len(papers)} ç¯‡')
        
        if not papers:
            print('âŒ æ— è®ºæ–‡éœ€è¦å¤„ç†')
            return
        
        # å¹¶å‘ç”ŸæˆTTS
        results = await process_batch(papers, args.voice, output_dir, args.concurrency)
        
        # ä¿å­˜æ•°æ®åº“è®°å½•
        save_tts_records(db, results, output_dir)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for _, success in results if success)
        print(f'\nğŸ“Š æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ {success_count}/{len(papers)}')
        
    except Exception as e:
        print(f'âŒ æ‰§è¡Œå¤±è´¥: {e}')
    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(main())
