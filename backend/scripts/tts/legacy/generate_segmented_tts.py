#!/usr/bin/env python3
"""
AIè§£è¯»åˆ†æ®µTTSç”Ÿæˆè„šæœ¬
å°†AIè§£è¯»å†…å®¹æŒ‰æ®µè½åˆ‡åˆ†ï¼Œç”ŸæˆOPUS 24kHzæ ¼å¼çš„éŸ³é¢‘ç‰‡æ®µ
"""

import argparse
import asyncio
import re
import sys
import subprocess
import json
from pathlib import Path
from typing import List, Dict, Tuple
from uuid import UUID

# æ·»åŠ backendæ ¹ç›®å½•åˆ°è·¯å¾„
backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

import edge_tts
from sqlalchemy import text
from app.db.session import SessionLocal


def clean_markdown_for_tts(text: str) -> str:
    """æ¸…ç†markdownè¯­æ³•ï¼Œä½¿å…¶é€‚åˆTTS"""
    if not text:
        return text
    
    # å¤„ç†JSONæ ¼å¼çš„å†…å®¹
    if text.strip().startswith('```json'):
        try:
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(1))
                content_parts = []
                for item in json_data:
                    if isinstance(item, dict) and 'zh' in item:
                        content_parts.append(item['zh'])
                text = '\n\n'.join(content_parts)
        except:
            pass
    
    # æ¸…ç†markdownè¯­æ³•
    text = re.sub(r'```[^`]*```', '', text)  # ä»£ç å—
    text = re.sub(r'`([^`]+)`', r'\1', text)  # è¡Œå†…ä»£ç 
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # åŠ ç²—
    text = re.sub(r'\*([^*]+)\*', r'\1', text)  # æ–œä½“
    text = re.sub(r'#{1,6}\s*', '', text)  # æ ‡é¢˜
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # é“¾æ¥
    text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)  # åˆ—è¡¨
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)  # æ•°å­—åˆ—è¡¨
    text = re.sub(r'\n{3,}', '\n\n', text)  # å¤šä½™æ¢è¡Œ
    
    return text.strip()


class AIInterpretationSegmenter:
    """AIè§£è¯»å†…å®¹åˆ†æ®µå™¨"""
    
    @staticmethod
    def segment_interpretation(content: str, target_segments: int = 6) -> List[Tuple[str, str]]:
        """
        å°†AIè§£è¯»å†…å®¹æŒ‰ç»“æ„åŒ–æ®µè½åˆ†å‰²ä¸ºæŒ‡å®šæ•°é‡çš„ç‰‡æ®µ
        
        Args:
            content: AIè§£è¯»å†…å®¹
            target_segments: ç›®æ ‡ç‰‡æ®µæ•°é‡
            
        Returns:
            List of (segment_type, text) tuples
        """
        segments = []
        
        # æ¸…ç†å†…å®¹
        content = content.strip()
        
        # å…ˆæŒ‰ä¸»è¦ç»“æ„åˆ†å‰²
        major_sections = []
        
        # æŒ‰æ ‡é¢˜å’Œé‡è¦æ ‡è®°åˆ†å‰²
        parts = re.split(r'(?=##\s)|(?=\*\*(?:æ ¸å¿ƒåˆ›æ–°ç‚¹|ä¸»è¦è´¡çŒ®|ç ”ç©¶èƒŒæ™¯|æ ¸å¿ƒæ–¹æ³•|å®éªŒç»“æœ|å­¦æœ¯ä»·å€¼)\*\*)', content)
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
            
            # è¯†åˆ«æ®µè½ç±»å‹
            if part.startswith('##'):
                title = re.sub(r'^##\s*', '', part).strip()
                major_sections.append(('title', title))
            elif any(keyword in part for keyword in ['æ ¸å¿ƒåˆ›æ–°ç‚¹', 'ä¸»è¦è´¡çŒ®', 'ç ”ç©¶èƒŒæ™¯', 'æ ¸å¿ƒæ–¹æ³•', 'å®éªŒç»“æœ', 'å­¦æœ¯ä»·å€¼']):
                major_sections.append(('key_section', part))
            else:
                major_sections.append(('content', part))
        
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾ç»“æ„ï¼ŒæŒ‰é•¿åº¦å‡åŒ€åˆ†å‰²
        if len(major_sections) <= 1:
            text_length = len(content)
            segment_length = text_length // target_segments
            
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
            current_segment = ""
            segment_count = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                if len(current_segment + sentence) > segment_length and current_segment and segment_count < target_segments - 1:
                    segments.append((f'part_{segment_count + 1}', current_segment.strip()))
                    current_segment = sentence + "ã€‚"
                    segment_count += 1
                else:
                    current_segment += sentence + "ã€‚"
            
            if current_segment:
                segments.append((f'part_{segment_count + 1}', current_segment.strip()))
        
        else:
            # åˆå¹¶å°æ®µè½ï¼Œç¡®ä¿è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(major_sections) > target_segments:
                # éœ€è¦åˆå¹¶
                segments_per_group = len(major_sections) // target_segments
                remainder = len(major_sections) % target_segments
                
                current_group = ""
                group_count = 0
                items_in_group = 0
                target_items = segments_per_group + (1 if group_count < remainder else 0)
                
                for section_type, text in major_sections:
                    if items_in_group >= target_items and group_count < target_segments - 1:
                        segments.append((f'section_{group_count + 1}', current_group.strip()))
                        current_group = text
                        group_count += 1
                        items_in_group = 1
                        target_items = segments_per_group + (1 if group_count < remainder else 0)
                    else:
                        if current_group:
                            current_group += "\n\n" + text
                        else:
                            current_group = text
                        items_in_group += 1
                
                if current_group:
                    segments.append((f'section_{group_count + 1}', current_group.strip()))
            
            elif len(major_sections) < target_segments:
                # éœ€è¦æ‹†åˆ†é•¿æ®µè½
                for section_type, text in major_sections:
                    if len(text) > 300:  # é•¿æ®µè½éœ€è¦æ‹†åˆ†
                        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
                        mid_point = len(sentences) // 2
                        
                        part1 = "ã€‚".join(sentences[:mid_point]).strip() + "ã€‚"
                        part2 = "ã€‚".join(sentences[mid_point:]).strip()
                        
                        segments.append((section_type + '_1', part1))
                        segments.append((section_type + '_2', part2))
                    else:
                        segments.append((section_type, text))
            
            else:
                # æ•°é‡åˆšå¥½
                segments = major_sections
        
        # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
        if len(segments) > target_segments:
            # åˆå¹¶æœ€åå‡ ä¸ªæ®µè½
            excess = len(segments) - target_segments
            if excess > 0:
                last_segments = segments[-(excess + 1):]
                combined_text = "\n\n".join([text for _, text in last_segments])
                segments = segments[:-(excess + 1)]
                segments.append(('final_section', combined_text))
        
        # ç¡®ä¿è‡³å°‘æœ‰ç›®æ ‡æ•°é‡çš„æ®µè½
        while len(segments) < target_segments and segments:
            # æ‰¾æœ€é•¿çš„æ®µè½è¿›è¡Œæ‹†åˆ†
            longest_idx = max(range(len(segments)), key=lambda i: len(segments[i][1]))
            section_type, text = segments[longest_idx]
            
            if len(text) > 100:  # åªæ‹†åˆ†è¶³å¤Ÿé•¿çš„æ®µè½
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
                if len(sentences) > 2:
                    mid_point = len(sentences) // 2
                    part1 = "ã€‚".join(sentences[:mid_point]).strip() + "ã€‚"
                    part2 = "ã€‚".join(sentences[mid_point:]).strip()
                    
                    segments[longest_idx] = (section_type + '_1', part1)
                    segments.insert(longest_idx + 1, (section_type + '_2', part2))
                else:
                    break
            else:
                break
        
        return segments[:target_segments]


async def generate_segment_tts(text: str, output_path: Path, voice: str = "zh-CN-XiaoxiaoNeural") -> bool:
    """ç”Ÿæˆå•ä¸ªç‰‡æ®µçš„TTSéŸ³é¢‘"""
    try:
        # æ¸…ç†markdownè¯­æ³•
        clean_text = clean_markdown_for_tts(text)
        
        # ç”ŸæˆTTS
        communicate = edge_tts.Communicate(clean_text, voice)
        
        # å…ˆç”ŸæˆWAVæ–‡ä»¶
        temp_wav = output_path.with_suffix('.wav')
        await communicate.save(str(temp_wav))
        
        # è½¬æ¢ä¸ºOPUS 24kHz
        cmd = [
            "ffmpeg",
            "-i", str(temp_wav),
            "-c:a", "libopus",
            "-ar", "24000",
            "-b:a", "20k",
            "-application", "voip",
            "-y",
            str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # åˆ é™¤ä¸´æ—¶WAVæ–‡ä»¶
        if temp_wav.exists():
            temp_wav.unlink()
        
        return True
        
    except Exception as e:
        print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
        return False


async def process_paper_interpretation(
    paper_id: UUID, 
    title_en: str,
    title_zh: str,
    interpretation: str, 
    output_dir: Path,
    voice: str = "zh-CN-XiaoxiaoNeural"
) -> Dict[str, any]:
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„AIè§£è¯»"""
    
    print(f"\nğŸµ å¤„ç†è®ºæ–‡: {paper_id}")
    print(f"  ğŸ“– æ ‡é¢˜: {title_zh}")
    
    # åˆ›å»ºè®ºæ–‡ä¸“ç”¨ç›®å½•
    paper_dir = output_dir / str(paper_id)
    paper_dir.mkdir(exist_ok=True)
    
    # å‡†å¤‡å®Œæ•´å†…å®¹ï¼ˆåŒ…å«æ ‡é¢˜æœ—è¯»ï¼‰
    full_content = f"""
è®ºæ–‡æ ‡é¢˜ï¼š{title_zh}

è‹±æ–‡æ ‡é¢˜ï¼š{title_en}

AIè§£è¯»ï¼š{interpretation}
    """.strip()
    
    # åˆ†æ®µ
    segmenter = AIInterpretationSegmenter()
    segments = segmenter.segment_interpretation(full_content, target_segments=6)
    
    print(f"  ğŸ“ åˆ†å‰²ä¸º {len(segments)} ä¸ªç‰‡æ®µ")
    
    results = {
        'paper_id': paper_id,
        'title_zh': title_zh,
        'title_en': title_en,
        'total_segments': len(segments),
        'successful_segments': 0,
        'failed_segments': 0,
        'segment_files': [],
        'total_size': 0
    }
    
    # ç”Ÿæˆæ¯ä¸ªç‰‡æ®µçš„éŸ³é¢‘
    for i, (segment_type, text) in enumerate(segments):
        segment_file = paper_dir / f"segment_{i:02d}_{segment_type}.opus"
        
        print(f"  ğŸ”„ ç‰‡æ®µ {i+1}/{len(segments)}: {segment_type} ({len(text)} å­—ç¬¦)")
        
        success = await generate_segment_tts(text, segment_file, voice)
        
        if success and segment_file.exists():
            file_size = segment_file.stat().st_size
            results['successful_segments'] += 1
            results['segment_files'].append({
                'index': i,
                'type': segment_type,
                'file': str(segment_file),
                'size': file_size,
                'text_length': len(text),
                'text_preview': text[:100] + '...' if len(text) > 100 else text
            })
            results['total_size'] += file_size
            print(f"    âœ… æˆåŠŸ: {file_size:,} bytes")
        else:
            results['failed_segments'] += 1
            print(f"    âŒ å¤±è´¥")
    
    # ç”Ÿæˆç‰‡æ®µç´¢å¼•æ–‡ä»¶
    index_file = paper_dir / "segments.json"
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump({
            'paper_id': str(paper_id),
            'title_zh': title_zh,
            'title_en': title_en,
            'total_segments': results['total_segments'],
            'segments': results['segment_files']
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  ğŸ“Š å®Œæˆ: {results['successful_segments']}/{results['total_segments']} ç‰‡æ®µ")
    print(f"  ğŸ’¾ æ€»å¤§å°: {results['total_size']:,} bytes")
    
    return results


async def main():
    parser = argparse.ArgumentParser(description="AIè§£è¯»åˆ†æ®µTTSç”Ÿæˆ")
    parser.add_argument("--batch-size", type=int, default=5, help="æ¯æ‰¹å¤„ç†æ•°é‡")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    parser.add_argument("--output-dir", default="backend/data/tts_segments", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--start-offset", type=int, default=0, help="èµ·å§‹åç§»")
    parser.add_argument("--source", default="all", help="æ•°æ®æº (all/neurips2025)")
    args = parser.parse_args()
    
    print(f"ğŸµ AIè§£è¯»åˆ†æ®µTTSç”Ÿæˆå™¨")
    print(f"é…ç½®: æ¯æ‰¹{args.batch_size}ç¯‡ï¼Œè¯­éŸ³æ¨¡å‹: {args.voice}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    db = SessionLocal()
    
    try:
        # æ„å»ºæŸ¥è¯¢
        if args.source == "neurips2025":
            query = text("""
                SELECT 
                    pi.paper_id, 
                    p.title,
                    COALESCE(pt.title_zh, p.title) as title_zh,
                    pi.interpretation
                FROM paper_interpretations pi
                JOIN papers p ON pi.paper_id = p.id
                LEFT JOIN paper_translations pt ON pi.paper_id = pt.paper_id
                WHERE pi.interpretation IS NOT NULL 
                AND LENGTH(pi.interpretation) > 100
                AND p.source = 'conf/neurips2025'
                ORDER BY pi.paper_id
                LIMIT :limit OFFSET :offset
            """)
        else:
            query = text("""
                SELECT 
                    pi.paper_id, 
                    p.title,
                    COALESCE(pt.title_zh, p.title) as title_zh,
                    pi.interpretation
                FROM paper_interpretations pi
                JOIN papers p ON pi.paper_id = p.id
                LEFT JOIN paper_translations pt ON pi.paper_id = pt.paper_id
                WHERE pi.interpretation IS NOT NULL 
                AND LENGTH(pi.interpretation) > 100
                ORDER BY pi.paper_id
                LIMIT :limit OFFSET :offset
            """)
        
        # è·å–è®ºæ–‡æ•°æ®
        result = db.execute(query, {"limit": args.batch_size, "offset": args.start_offset})
        papers = [(
            row[0] if isinstance(row[0], UUID) else UUID(row[0]), 
            row[1], 
            row[2], 
            row[3]
        ) for row in result.fetchall()]
        
        if not papers:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„è®ºæ–‡")
            return
        
        print(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡
        total_results = {
            'processed_papers': 0,
            'total_segments': 0,
            'successful_segments': 0,
            'total_size': 0
        }
        
        for paper_id, title_en, title_zh, interpretation in papers:
            try:
                result = await process_paper_interpretation(
                    paper_id, title_en, title_zh, interpretation, output_dir, args.voice
                )
                
                total_results['processed_papers'] += 1
                total_results['total_segments'] += result['total_segments']
                total_results['successful_segments'] += result['successful_segments']
                total_results['total_size'] += result['total_size']
                
            except Exception as e:
                print(f"âŒ å¤„ç†è®ºæ–‡ {paper_id} å¤±è´¥: {e}")
                continue
        
        # è¾“å‡ºæ€»ç»“
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡:")
        print(f"  å¤„ç†è®ºæ–‡: {total_results['processed_papers']}")
        print(f"  æ€»ç‰‡æ®µæ•°: {total_results['total_segments']}")
        print(f"  æˆåŠŸç‰‡æ®µ: {total_results['successful_segments']}")
        print(f"  æˆåŠŸç‡: {total_results['successful_segments']/total_results['total_segments']*100:.1f}%")
        print(f"  æ€»å¤§å°: {total_results['total_size']:,} bytes ({total_results['total_size']/1024/1024:.1f} MB)")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


if __name__ == "__main__":
    asyncio.run(main())
