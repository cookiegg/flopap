#!/usr/bin/env python3
"""
æ··åˆç­–ç•¥TTSç”Ÿæˆå™¨
ç»“åˆå•æœºä¼˜åŒ–å’Œåˆ†å¸ƒå¼å¤„ç†
"""

import argparse
import asyncio
import json
import time
from pathlib import Path
from typing import List, Tuple
from uuid import UUID

# æ·»åŠ backendæ ¹ç›®å½•åˆ°è·¯å¾„
backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

from sqlalchemy import text
from app.db.session import SessionLocal
from large_batch_tts_manager import LargeBatchTTSManager

# å¯¼å…¥åˆ†æ®µTTSå‡½æ•°
sys.path.append(str(backend_root / "scripts" / "tts"))
from generate_segmented_tts import process_paper_interpretation


class DistributedTTSCoordinator:
    """åˆ†å¸ƒå¼TTSåè°ƒå™¨"""
    
    def __init__(self, machine_id: int, total_machines: int):
        self.machine_id = machine_id
        self.total_machines = total_machines
        self.progress_file = Path(f"tts_progress_machine_{machine_id}.json")
    
    def get_machine_papers(self, all_papers: List, start_offset: int = 0) -> List:
        """è·å–å½“å‰æœºå™¨è´Ÿè´£çš„è®ºæ–‡"""
        # è·³è¿‡start_offsetï¼Œç„¶åæŒ‰æœºå™¨æ•°é‡åˆ†é…
        papers_after_offset = all_papers[start_offset:]
        
        # æ¯å°æœºå™¨å¤„ç†çš„è®ºæ–‡
        machine_papers = []
        for i, paper in enumerate(papers_after_offset):
            if i % self.total_machines == self.machine_id:
                machine_papers.append(paper)
        
        return machine_papers
    
    async def run_distributed_generation(
        self,
        output_dir: Path,
        voice: str = "zh-CN-XiaoxiaoNeural",
        start_offset: int = 0,
        total_limit: int = 1000
    ):
        """è¿è¡Œåˆ†å¸ƒå¼ç”Ÿæˆ"""
        
        print(f"ğŸ¤– æœºå™¨ {self.machine_id + 1}/{self.total_machines} å¯åŠ¨")
        print(f"ğŸ“Š å¤„ç†èŒƒå›´: ä»ç¬¬ {start_offset} ç¯‡å¼€å§‹ï¼Œæ€»å…± {total_limit} ç¯‡")
        
        # è·å–æ‰€æœ‰è®ºæ–‡æ•°æ®
        db = SessionLocal()
        try:
            query = text("""
                SELECT 
                    pi.paper_id, 
                    p.title,
                    COALESCE(pt.title_zh, p.title) as title_zh,
                    pi.interpretation
                FROM paper_interpretations pi
                JOIN papers p ON pi.paper_id = p.id
                LEFT JOIN paper_translations pt ON pi.paper_id = pt.paper_id
                WHERE pi.interpretation IS NOT NULL 
                AND LENGTH(pi.interpretation) > 100
                ORDER BY pi.paper_id
                LIMIT :limit OFFSET :offset
            """)
            
            result = db.execute(query, {"limit": total_limit, "offset": start_offset})
            all_papers = [(
                row[0] if isinstance(row[0], UUID) else UUID(row[0]), 
                row[1], 
                row[2], 
                row[3]
            ) for row in result.fetchall()]
            
        finally:
            db.close()
        
        # è·å–å½“å‰æœºå™¨è´Ÿè´£çš„è®ºæ–‡
        machine_papers = self.get_machine_papers(all_papers)
        
        print(f"ğŸ“š å½“å‰æœºå™¨è´Ÿè´£: {len(machine_papers)} ç¯‡è®ºæ–‡")
        
        if not machine_papers:
            print("âŒ å½“å‰æœºå™¨æ— åˆ†é…è®ºæ–‡")
            return
        
        # åˆ›å»ºTTSç®¡ç†å™¨
        manager = LargeBatchTTSManager(
            max_concurrent_papers=2,
            max_concurrent_segments=6,
            retry_attempts=3,
            progress_file=str(self.progress_file)
        )
        
        # å¼€å§‹ç”Ÿæˆ
        await manager.generate_large_batch(
            machine_papers,
            output_dir,
            voice,
            batch_size=5  # å°æ‰¹æ¬¡ï¼Œæ›´å¥½çš„é”™è¯¯æ¢å¤
        )


async def main():
    parser = argparse.ArgumentParser(description="åˆ†å¸ƒå¼å¤§è§„æ¨¡TTSç”Ÿæˆ")
    parser.add_argument("--machine-id", type=int, default=0, help="æœºå™¨ID (0-based)")
    parser.add_argument("--total-machines", type=int, default=1, help="æ€»æœºå™¨æ•°")
    parser.add_argument("--start-offset", type=int, default=0, help="èµ·å§‹åç§»")
    parser.add_argument("--total-limit", type=int, default=1000, help="æ€»è®ºæ–‡æ•°")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    parser.add_argument("--output-dir", default="backend/data/tts_segments_large", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºåˆ†å¸ƒå¼åè°ƒå™¨
    coordinator = DistributedTTSCoordinator(args.machine_id, args.total_machines)
    
    # è¿è¡Œåˆ†å¸ƒå¼ç”Ÿæˆ
    await coordinator.run_distributed_generation(
        output_dir=output_dir,
        voice=args.voice,
        start_offset=args.start_offset,
        total_limit=args.total_limit
    )


if __name__ == "__main__":
    asyncio.run(main())
