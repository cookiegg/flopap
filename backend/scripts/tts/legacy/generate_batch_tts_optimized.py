#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„æ‰¹é‡TTSç”Ÿæˆè„šæœ¬
åŸºäºfinal_fix.pyçš„æˆåŠŸç»éªŒï¼Œé€‚ç”¨äºä»»æ„è®ºæ–‡æ¥æº
"""

import asyncio
import argparse
import subprocess
import json
import uuid
import random
import re
from pathlib import Path
from typing import List, Tuple
from uuid import UUID
import sys

backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

import edge_tts
from sqlalchemy import text
from app.db.session import SessionLocal


def clean_markdown_for_tts(text: str) -> str:
    """æ¸…ç†markdownè¯­æ³•"""
    if not text:
        return text
    
    if text.strip().startswith('```json'):
        try:
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(1))
                content_parts = []
                for item in json_data:
                    if isinstance(item, dict) and 'zh' in item:
                        content_parts.append(item['zh'])
                text = '\n\n'.join(content_parts)
        except:
            pass
    
    text = re.sub(r'```[^`]*```', '', text)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'#{1,6}\s*', '', text)
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def segment_interpretation(content: str, target_segments: int = 6) -> List[Tuple[str, str]]:
    """å°†å†…å®¹åˆ†å‰²ä¸º6ä¸ªç‰‡æ®µï¼Œä¿®æ­£ç‰ˆæœ¬"""
    segments = []
    content = content.strip()
    
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if len(sentences) == 0:
        # å¦‚æœæ²¡æœ‰å¥å­ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
        chars_per_segment = len(content) // target_segments
        for i in range(target_segments):
            start = i * chars_per_segment
            end = start + chars_per_segment if i < target_segments - 1 else len(content)
            segment_text = content[start:end]
            if segment_text.strip():
                segments.append((f'part_{i+1}', segment_text))
            else:
                segments.append((f'part_{i+1}', 'è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„ç‰‡æ®µã€‚'))
    elif len(sentences) <= target_segments:
        # å¥å­æ•°å°‘äºæˆ–ç­‰äºç›®æ ‡ç‰‡æ®µæ•°
        for i, sentence in enumerate(sentences):
            segments.append((f'part_{i+1}', sentence + 'ã€‚'))
        
        # è¡¥å……å‰©ä½™ç‰‡æ®µï¼Œä½¿ç”¨æœ€åä¸€å¥çš„é‡å¤æˆ–ç®€çŸ­æ–‡æœ¬
        last_sentence = sentences[-1] if sentences else "è¿™æ˜¯è¡¥å……å†…å®¹"
        while len(segments) < target_segments:
            segments.append((f'part_{len(segments)+1}', f'{last_sentence}ã€‚'))
    else:
        # å¥å­æ•°å¤šäºç›®æ ‡ç‰‡æ®µæ•°ï¼Œæ­£å¸¸åˆ†ç»„
        sentences_per_segment = len(sentences) // target_segments
        remainder = len(sentences) % target_segments
        
        start_idx = 0
        for i in range(target_segments):
            segment_size = sentences_per_segment + (1 if i < remainder else 0)
            segment_sentences = sentences[start_idx:start_idx + segment_size]
            segment_text = 'ã€‚'.join(segment_sentences) + 'ã€‚'
            segments.append((f'part_{i+1}', segment_text))
            start_idx += segment_size
    
    return segments[:target_segments]


async def generate_segment_tts(text: str, output_path: Path, voice: str = "zh-CN-XiaoxiaoNeural") -> bool:
    """ç”Ÿæˆå•ä¸ªç‰‡æ®µçš„TTSéŸ³é¢‘"""
    try:
        if not text.strip():
            return False
        
        await asyncio.sleep(random.uniform(0.3, 0.8))
        
        clean_text = clean_markdown_for_tts(text)
        if len(clean_text) > 800:
            clean_text = clean_text[:800] + "ã€‚"
        
        communicate = edge_tts.Communicate(clean_text, voice)
        
        temp_wav = output_path.parent / f"temp_{uuid.uuid4().hex[:8]}.wav"
        await communicate.save(str(temp_wav))
        
        cmd = [
            "ffmpeg", "-i", str(temp_wav),
            "-c:a", "libopus", "-ar", "24000", "-b:a", "20k",
            "-application", "voip", "-y", str(output_path)
        ]
        
        subprocess.run(cmd, capture_output=True, check=True)
        
        if temp_wav.exists():
            temp_wav.unlink()
        
        return output_path.exists() and output_path.stat().st_size > 0
        
    except Exception as e:
        print(f"    âŒ ç”Ÿæˆå¤±è´¥: {e}")
        if 'temp_wav' in locals() and temp_wav.exists():
            temp_wav.unlink()
        return False


async def process_paper(paper_id: UUID, title_en: str, title_zh: str, interpretation: str, output_dir: Path, voice: str) -> dict:
    """å¤„ç†å•ç¯‡è®ºæ–‡"""
    paper_dir = output_dir / str(paper_id)
    paper_dir.mkdir(exist_ok=True)
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶
    existing_count = 0
    for i in range(6):
        segment_file = paper_dir / f"segment_{i:02d}_part_{i+1}.opus"
        if segment_file.exists() and segment_file.stat().st_size > 0:
            existing_count += 1
    
    if existing_count == 6:
        return {'status': 'skipped', 'generated': 0}
    
    # å‡†å¤‡å†…å®¹å¹¶åˆ†æ®µ
    full_content = f"è®ºæ–‡æ ‡é¢˜ï¼š{title_zh}\nè‹±æ–‡æ ‡é¢˜ï¼š{title_en}\nAIè§£è¯»ï¼š{interpretation}"
    segments = segment_interpretation(full_content, target_segments=6)
    
    generated = 0
    for i, (segment_type, segment_text) in enumerate(segments):
        segment_file = paper_dir / f"segment_{i:02d}_{segment_type}.opus"
        
        # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if segment_file.exists() and segment_file.stat().st_size > 0:
            continue
        
        if await generate_segment_tts(segment_text, segment_file, voice):
            generated += 1
    
    return {'status': 'processed', 'generated': generated}


async def main():
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„æ‰¹é‡TTSç”Ÿæˆ")
    parser.add_argument("--source", required=True, help="è®ºæ–‡æ¥æº (å¦‚: conf/iclr2024)")
    parser.add_argument("--concurrency", type=int, default=6, help="å¹¶å‘æ•°")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    parser.add_argument("--output-dir", default="backend/data/tts", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--batch-size", type=int, default=100, help="æ‰¹å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    print(f"ğŸµ æ‰¹é‡TTSç”Ÿæˆå¯åŠ¨")
    print(f"ğŸ“š è®ºæ–‡æ¥æº: {args.source}")
    print(f"âš¡ å¹¶å‘æ•°: {args.concurrency}")
    print(f"ğŸ¤ è¯­éŸ³æ¨¡å‹: {args.voice}")
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–è®ºæ–‡æ•°æ®
    db = SessionLocal()
    try:
        query = text("""
            SELECT 
                pi.paper_id, 
                p.title,
                COALESCE(pt.title_zh, p.title) as title_zh,
                pi.interpretation
            FROM paper_interpretations pi
            JOIN papers p ON pi.paper_id = p.id
            LEFT JOIN paper_translations pt ON pi.paper_id = pt.paper_id
            WHERE pi.interpretation IS NOT NULL 
            AND LENGTH(pi.interpretation) > 50
            AND p.source = :source
            ORDER BY pi.paper_id
        """)
        
        result = db.execute(query, {"source": args.source})
        papers = [(
            row[0] if isinstance(row[0], UUID) else UUID(row[0]), 
            row[1], row[2], row[3]
        ) for row in result.fetchall()]
        
    finally:
        db.close()
    
    if not papers:
        print(f"âŒ æœªæ‰¾åˆ°æ¥æºä¸º '{args.source}' çš„è®ºæ–‡")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # åˆ†æ‰¹å¤„ç†
    total_processed = 0
    total_generated = 0
    
    semaphore = asyncio.Semaphore(args.concurrency)
    
    async def process_with_semaphore(paper_data):
        async with semaphore:
            paper_id, title_en, title_zh, interpretation = paper_data
            return await process_paper(paper_id, title_en, title_zh, interpretation, output_dir, args.voice)
    
    for i in range(0, len(papers), args.batch_size):
        batch = papers[i:i + args.batch_size]
        print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {i//args.batch_size + 1}/{(len(papers) + args.batch_size - 1)//args.batch_size}")
        print(f"ğŸ“ å½“å‰æ‰¹æ¬¡: {len(batch)} ç¯‡è®ºæ–‡")
        
        tasks = [process_with_semaphore(paper_data) for paper_data in batch]
        results = await asyncio.gather(*tasks)
        
        batch_processed = sum(1 for r in results if r['status'] == 'processed')
        batch_generated = sum(r['generated'] for r in results)
        
        total_processed += batch_processed
        total_generated += batch_generated
        
        print(f"âœ… æ‰¹æ¬¡å®Œæˆ: å¤„ç† {batch_processed} ç¯‡ï¼Œç”Ÿæˆ {batch_generated} ä¸ªç‰‡æ®µ")
    
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  æ€»è®ºæ–‡æ•°: {len(papers)}")
    print(f"  å¤„ç†è®ºæ–‡: {total_processed}")
    print(f"  ç”Ÿæˆç‰‡æ®µ: {total_generated}")


if __name__ == "__main__":
    asyncio.run(main())
