#!/usr/bin/env python3
"""
åˆ†æ‰¹ä¸ºNeurIPS 2025è®ºæ–‡ç”ŸæˆTTSè¯­éŸ³

ç”¨æ³•ï¼š
  python scripts/tts/generate_neurips_tts_batch.py --batch-size 10 --max-workers 10
"""
import argparse
import sys
from pathlib import Path
from uuid import UUID

# æ·»åŠ backendæ ¹ç›®å½•åˆ°è·¯å¾„
backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

from sqlalchemy import text
from app.db.session import SessionLocal
from app.services.content_generation.tts_generate import batch_generate_tts_with_storage


def get_all_neurips_papers(session, offset: int = 0, limit: int = 10):
    """è·å–NeurIPS 2025è®ºæ–‡IDï¼ˆåˆ†é¡µï¼‰"""
    query = text("""
        SELECT p.id
        FROM papers p
        JOIN paper_translations pt ON p.id = pt.paper_id
        JOIN paper_interpretations pi ON p.id = pi.paper_id
        WHERE p.source = 'conf/neurips2025'
        AND pt.title_zh IS NOT NULL 
        AND pi.interpretation IS NOT NULL
        ORDER BY p.id
        LIMIT :limit OFFSET :offset
    """)
    
    result = session.execute(query, {"limit": limit, "offset": offset})
    return [row[0] for row in result.fetchall()]


def main():
    parser = argparse.ArgumentParser(description="åˆ†æ‰¹ä¸ºNeurIPS 2025è®ºæ–‡ç”ŸæˆTTSè¯­éŸ³")
    parser.add_argument("--batch-size", type=int, default=10, help="æ¯æ‰¹å¤„ç†æ•°é‡")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    parser.add_argument("--max-workers", type=int, default=10, help="å¹¶å‘æ•°")
    parser.add_argument("--start-batch", type=int, default=0, help="èµ·å§‹æ‰¹æ¬¡")
    args = parser.parse_args()
    
    print(f"ğŸµ å¼€å§‹åˆ†æ‰¹ä¸ºNeurIPS 2025è®ºæ–‡ç”ŸæˆTTSè¯­éŸ³")
    print(f"é…ç½®: æ¯æ‰¹{args.batch_size}ç¯‡ï¼Œ{args.max_workers}ä¸ªå¹¶å‘ï¼Œè¯­éŸ³æ¨¡å‹: {args.voice}")
    
    db = SessionLocal()
    
    try:
        # è·å–æ€»æ•°
        total_query = text("""
            SELECT COUNT(*)
            FROM papers p
            JOIN paper_translations pt ON p.id = pt.paper_id
            JOIN paper_interpretations pi ON p.id = pi.paper_id
            WHERE p.source = 'conf/neurips2025'
            AND pt.title_zh IS NOT NULL 
            AND pi.interpretation IS NOT NULL
        """)
        total_count = db.execute(total_query).scalar()
        total_batches = (total_count + args.batch_size - 1) // args.batch_size
        
        print(f"æ€»è®¡: {total_count}ç¯‡è®ºæ–‡ï¼Œ{total_batches}æ‰¹æ¬¡")
        
        success_count = 0
        
        for batch_num in range(args.start_batch, total_batches):
            offset = batch_num * args.batch_size
            print(f"\nğŸ“¦ å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹ (åç§»: {offset})")
            
            # è·å–å½“å‰æ‰¹æ¬¡çš„è®ºæ–‡ID
            paper_ids = get_all_neurips_papers(db, offset, args.batch_size)
            
            if not paper_ids:
                print("âŒ å½“å‰æ‰¹æ¬¡æ— å¯ç”¨è®ºæ–‡")
                continue
            
            print(f"âœ… å½“å‰æ‰¹æ¬¡: {len(paper_ids)} ç¯‡è®ºæ–‡")
            
            # ç”ŸæˆTTS
            try:
                tts_file_paths = batch_generate_tts_with_storage(
                    session=db,
                    paper_ids=paper_ids,
                    voice=args.voice,
                    max_workers=args.max_workers,
                    save_to_storage=True
                )
                
                batch_success = len(tts_file_paths)
                success_count += batch_success
                
                print(f"ğŸ‰ ç¬¬ {batch_num + 1} æ‰¹å®Œæˆ: {batch_success}/{len(paper_ids)} ä¸ªæ–‡ä»¶")
                print(f"ğŸ“Š æ€»è¿›åº¦: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
                
            except Exception as e:
                print(f"âŒ ç¬¬ {batch_num + 1} æ‰¹å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
        print(f"æˆåŠŸç”Ÿæˆ: {success_count}/{total_count} ä¸ªTTSæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


if __name__ == "__main__":
    main()
