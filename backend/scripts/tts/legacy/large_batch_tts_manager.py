#!/usr/bin/env python3
"""
å¤§è§„æ¨¡TTSç”Ÿæˆç®¡ç†å™¨
æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€é”™è¯¯é‡è¯•ã€è¿›åº¦ç›‘æ§
"""

import asyncio
import time
import json
from pathlib import Path
from typing import List, Dict, Set
from uuid import UUID
from dataclasses import dataclass, asdict

@dataclass
class BatchProgress:
    """æ‰¹æ¬¡è¿›åº¦è·Ÿè¸ª"""
    total_papers: int = 0
    completed_papers: int = 0
    failed_papers: int = 0
    current_batch: int = 0
    total_batches: int = 0
    start_time: float = 0
    estimated_remaining: float = 0
    completed_paper_ids: Set[str] = None
    failed_paper_ids: Set[str] = None
    
    def __post_init__(self):
        if self.completed_paper_ids is None:
            self.completed_paper_ids = set()
        if self.failed_paper_ids is None:
            self.failed_paper_ids = set()


class LargeBatchTTSManager:
    """å¤§è§„æ¨¡TTSç”Ÿæˆç®¡ç†å™¨"""
    
    def __init__(self, 
                 max_concurrent_papers: int = 2,  # åŒæ—¶å¤„ç†çš„è®ºæ–‡æ•°
                 max_concurrent_segments: int = 6,  # æ¯ç¯‡è®ºæ–‡çš„ç‰‡æ®µå¹¶å‘æ•°
                 retry_attempts: int = 3,
                 progress_file: str = "tts_progress.json"):
        
        self.max_concurrent_papers = max_concurrent_papers
        self.max_concurrent_segments = max_concurrent_segments
        self.retry_attempts = retry_attempts
        self.progress_file = Path(progress_file)
        
        # å…¨å±€å¹¶å‘æ§åˆ¶ (2è®ºæ–‡ Ã— 6ç‰‡æ®µ = 12å¹¶å‘ï¼Œç•¥è¶…10ä½†åœ¨å¯æ§èŒƒå›´)
        self.global_semaphore = asyncio.Semaphore(max_concurrent_papers * max_concurrent_segments)
        
        self.progress = BatchProgress()
        self.load_progress()
    
    def load_progress(self):
        """åŠ è½½è¿›åº¦æ–‡ä»¶"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r') as f:
                    data = json.load(f)
                    self.progress.completed_paper_ids = set(data.get('completed_paper_ids', []))
                    self.progress.failed_paper_ids = set(data.get('failed_paper_ids', []))
                    self.progress.completed_papers = len(self.progress.completed_paper_ids)
                    self.progress.failed_papers = len(self.progress.failed_paper_ids)
                    print(f"ğŸ“‚ åŠ è½½è¿›åº¦: å·²å®Œæˆ {self.progress.completed_papers} ç¯‡")
            except Exception as e:
                print(f"âš ï¸  è¿›åº¦æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    
    def save_progress(self):
        """ä¿å­˜è¿›åº¦æ–‡ä»¶"""
        try:
            data = {
                'total_papers': self.progress.total_papers,
                'completed_papers': self.progress.completed_papers,
                'failed_papers': self.progress.failed_papers,
                'current_batch': self.progress.current_batch,
                'completed_paper_ids': list(self.progress.completed_paper_ids),
                'failed_paper_ids': list(self.progress.failed_paper_ids),
                'timestamp': time.time()
            }
            with open(self.progress_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            print(f"âš ï¸  è¿›åº¦ä¿å­˜å¤±è´¥: {e}")
    
    async def process_single_paper_with_retry(self, paper_data, output_dir, voice):
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
        paper_id, title_en, title_zh, interpretation = paper_data
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if str(paper_id) in self.progress.completed_paper_ids:
            return True
        
        for attempt in range(self.retry_attempts):
            try:
                # ä½¿ç”¨å…¨å±€ä¿¡å·é‡æ§åˆ¶æ€»å¹¶å‘æ•°
                async with self.global_semaphore:
                    # è¿™é‡Œè°ƒç”¨ä½ çš„åˆ†æ®µTTSç”Ÿæˆå‡½æ•°
                    from generate_segmented_tts import process_paper_interpretation
                    
                    result = await process_paper_interpretation(
                        paper_id, title_en, title_zh, interpretation, output_dir, voice
                    )
                    
                    if result and result['successful_segments'] > 0:
                        self.progress.completed_paper_ids.add(str(paper_id))
                        self.progress.completed_papers += 1
                        return True
                    
            except Exception as e:
                print(f"  âŒ è®ºæ–‡ {paper_id} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        self.progress.failed_paper_ids.add(str(paper_id))
        self.progress.failed_papers += 1
        return False
    
    async def process_batch(self, papers_batch, output_dir, voice):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„è®ºæ–‡"""
        # é™åˆ¶åŒæ—¶å¤„ç†çš„è®ºæ–‡æ•°é‡
        semaphore = asyncio.Semaphore(self.max_concurrent_papers)
        
        async def limited_process(paper_data):
            async with semaphore:
                return await self.process_single_paper_with_retry(paper_data, output_dir, voice)
        
        # å¹¶å‘å¤„ç†æ‰¹æ¬¡å†…çš„è®ºæ–‡
        tasks = [limited_process(paper) for paper in papers_batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return sum(1 for r in results if r is True)
    
    async def generate_large_batch(self, papers, output_dir, voice, batch_size=10):
        """å¤§è§„æ¨¡æ‰¹é‡ç”Ÿæˆ"""
        self.progress.total_papers = len(papers)
        self.progress.start_time = time.time()
        
        # è¿‡æ»¤å·²å®Œæˆçš„è®ºæ–‡
        remaining_papers = [
            p for p in papers 
            if str(p[0]) not in self.progress.completed_paper_ids
        ]
        
        if not remaining_papers:
            print("âœ… æ‰€æœ‰è®ºæ–‡éƒ½å·²å®Œæˆ!")
            return
        
        print(f"ğŸµ å¼€å§‹å¤§è§„æ¨¡TTSç”Ÿæˆ")
        print(f"æ€»è®ºæ–‡æ•°: {len(papers)}")
        print(f"å‰©ä½™è®ºæ–‡: {len(remaining_papers)}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"å¹¶å‘é…ç½®: {self.max_concurrent_papers}è®ºæ–‡ Ã— {self.max_concurrent_segments}ç‰‡æ®µ")
        
        # åˆ†æ‰¹å¤„ç†
        batches = [remaining_papers[i:i+batch_size] for i in range(0, len(remaining_papers), batch_size)]
        self.progress.total_batches = len(batches)
        
        for batch_idx, batch in enumerate(batches):
            self.progress.current_batch = batch_idx + 1
            
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} ({len(batch)} ç¯‡è®ºæ–‡)")
            
            batch_start = time.time()
            success_count = await self.process_batch(batch, output_dir, voice)
            batch_time = time.time() - batch_start
            
            # æ›´æ–°è¿›åº¦
            self.save_progress()
            
            # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
            if self.progress.completed_papers > 0:
                avg_time_per_paper = (time.time() - self.progress.start_time) / self.progress.completed_papers
                remaining_papers_count = self.progress.total_papers - self.progress.completed_papers
                self.progress.estimated_remaining = avg_time_per_paper * remaining_papers_count
            
            print(f"  âœ… æ‰¹æ¬¡å®Œæˆ: {success_count}/{len(batch)} ç¯‡æˆåŠŸ")
            print(f"  â±ï¸  æ‰¹æ¬¡è€—æ—¶: {batch_time:.1f}s")
            print(f"  ğŸ“Š æ€»è¿›åº¦: {self.progress.completed_papers}/{self.progress.total_papers} ({self.progress.completed_papers/self.progress.total_papers*100:.1f}%)")
            
            if self.progress.estimated_remaining > 0:
                print(f"  ğŸ• é¢„ä¼°å‰©ä½™: {self.progress.estimated_remaining/3600:.1f}å°æ—¶")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
            if batch_idx < len(batches) - 1:
                print("  ğŸ˜´ æ‰¹æ¬¡é—´ä¼‘æ¯ 30ç§’...")
                await asyncio.sleep(30)
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - self.progress.start_time
        print(f"\nğŸ‰ å¤§è§„æ¨¡ç”Ÿæˆå®Œæˆ!")
        print(f"æ€»è€—æ—¶: {total_time/3600:.1f}å°æ—¶")
        print(f"æˆåŠŸ: {self.progress.completed_papers} ç¯‡")
        print(f"å¤±è´¥: {self.progress.failed_papers} ç¯‡")
        print(f"æˆåŠŸç‡: {self.progress.completed_papers/(self.progress.completed_papers+self.progress.failed_papers)*100:.1f}%")


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    manager = LargeBatchTTSManager(
        max_concurrent_papers=2,    # åŒæ—¶å¤„ç†2ç¯‡è®ºæ–‡
        max_concurrent_segments=6,  # æ¯ç¯‡6ä¸ªç‰‡æ®µ
        retry_attempts=3
    )
    
    # è·å–1000ç¯‡è®ºæ–‡æ•°æ®
    # papers = get_papers_data(1000)
    
    # å¼€å§‹æ‰¹é‡ç”Ÿæˆ
    # await manager.generate_large_batch(
    #     papers, 
    #     output_dir=Path("data/tts_segments"), 
    #     voice="zh-CN-XiaoxiaoNeural",
    #     batch_size=10
    # )

if __name__ == "__main__":
    asyncio.run(main())
