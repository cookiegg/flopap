#!/usr/bin/env python3
"""
æ–¹æ¡ˆ2ï¼š6ä¸ªè„šæœ¬Ã—12å¹¶å‘å¤„ç†NeurIPS 2025è®ºæ–‡TTSç”Ÿæˆ
åŸºäºgenerate_neurips_tts_batch.pyæ”¹é€ ï¼Œæ”¯æŒåˆ†æ®µOPUSéŸ³é¢‘ç”Ÿæˆ
"""

import asyncio
import argparse
import re
import sys
import subprocess
import json
from pathlib import Path
from typing import List, Dict, Tuple
from uuid import UUID

backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

import edge_tts
from sqlalchemy import text
from app.db.session import SessionLocal


def clean_markdown_for_tts(text: str) -> str:
    """æ¸…ç†markdownè¯­æ³•ï¼Œä½¿å…¶é€‚åˆTTS"""
    if not text:
        return text
    
    if text.strip().startswith('```json'):
        try:
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
            if json_match:
                json_data = json.loads(json_match.group(1))
                content_parts = []
                for item in json_data:
                    if isinstance(item, dict) and 'zh' in item:
                        content_parts.append(item['zh'])
                text = '\n\n'.join(content_parts)
        except:
            pass
    
    text = re.sub(r'```[^`]*```', '', text)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'#{1,6}\s*', '', text)
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def segment_interpretation(content: str, target_segments: int = 6) -> List[Tuple[str, str]]:
    """å°†AIè§£è¯»å†…å®¹åˆ†å‰²ä¸º6ä¸ªç‰‡æ®µ"""
    segments = []
    content = content.strip()
    
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if len(sentences) <= target_segments:
        for i, sentence in enumerate(sentences):
            segments.append((f'part_{i+1}', sentence + 'ã€‚'))
    else:
        sentences_per_segment = len(sentences) // target_segments
        remainder = len(sentences) % target_segments
        
        start_idx = 0
        for i in range(target_segments):
            segment_size = sentences_per_segment + (1 if i < remainder else 0)
            segment_sentences = sentences[start_idx:start_idx + segment_size]
            segment_text = 'ã€‚'.join(segment_sentences) + 'ã€‚'
            segments.append((f'part_{i+1}', segment_text))
            start_idx += segment_size
    
    while len(segments) < target_segments:
        segments.append((f'part_{len(segments)+1}', ''))
    
    return segments[:target_segments]


async def generate_segment_tts(text: str, output_path: Path, voice: str = "zh-CN-XiaoxiaoNeural") -> bool:
    """ç”Ÿæˆå•ä¸ªç‰‡æ®µçš„TTSéŸ³é¢‘"""
    try:
        if not text.strip():
            return False
            
        clean_text = clean_markdown_for_tts(text)
        communicate = edge_tts.Communicate(clean_text, voice)
        
        # ä½¿ç”¨å”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶åé¿å…å†²çª
        import uuid
        temp_wav = output_path.parent / f"temp_{uuid.uuid4().hex[:8]}.wav"
        await communicate.save(str(temp_wav))
        
        cmd = [
            "ffmpeg", "-i", str(temp_wav),
            "-c:a", "libopus", "-ar", "24000", "-b:a", "20k",
            "-application", "voip", "-y", str(output_path)
        ]
        
        subprocess.run(cmd, capture_output=True, check=True)
        
        # åˆ é™¤ä¸´æ—¶WAVæ–‡ä»¶
        if temp_wav.exists():
            temp_wav.unlink()
        
        return True
        
    except Exception as e:
        print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„ä¸´æ—¶æ–‡ä»¶
        if 'temp_wav' in locals() and temp_wav.exists():
            temp_wav.unlink()
        return False


async def process_paper(paper_id: UUID, title_en: str, title_zh: str, interpretation: str, output_dir: Path, voice: str) -> Dict:
    """å¤„ç†å•ç¯‡è®ºæ–‡"""
    print(f"\nğŸµ å¤„ç†è®ºæ–‡: {paper_id}")
    print(f"  ğŸ“– æ ‡é¢˜: {title_zh}")
    
    paper_dir = output_dir / str(paper_id)
    paper_dir.mkdir(exist_ok=True)
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶
    existing_files = []
    for i in range(6):
        segment_file = paper_dir / f"segment_{i:02d}_part_{i+1}.opus"
        if segment_file.exists() and segment_file.stat().st_size > 0:
            existing_files.append(i)
    
    if len(existing_files) == 6:
        total_size = sum((paper_dir / f"segment_{i:02d}_part_{i+1}.opus").stat().st_size for i in range(6))
        print(f"  âœ… å·²å­˜åœ¨å®Œæ•´éŸ³é¢‘æ–‡ä»¶ï¼Œè·³è¿‡ç”Ÿæˆ")
        return {'successful': 6, 'total': 6, 'size': total_size}
    
    full_content = f"è®ºæ–‡æ ‡é¢˜ï¼š{title_zh}\nè‹±æ–‡æ ‡é¢˜ï¼š{title_en}\nAIè§£è¯»ï¼š{interpretation}"
    
    segments = segment_interpretation(full_content, target_segments=6)
    print(f"  ğŸ“ åˆ†å‰²ä¸º {len(segments)} ä¸ªç‰‡æ®µ")
    
    results = {'successful': len(existing_files), 'total': len(segments), 'size': 0}
    
    for i, (segment_type, text) in enumerate(segments):
        segment_file = paper_dir / f"segment_{i:02d}_{segment_type}.opus"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if i in existing_files:
            file_size = segment_file.stat().st_size
            results['size'] += file_size
            print(f"  â­ï¸  ç‰‡æ®µ {i+1}/{len(segments)}: å·²å­˜åœ¨ ({file_size:,} bytes)")
            continue
        
        print(f"  ğŸ”„ ç‰‡æ®µ {i+1}/{len(segments)}: {len(text)} å­—ç¬¦")
        
        success = await generate_segment_tts(text, segment_file, voice)
        
        if success and segment_file.exists():
            file_size = segment_file.stat().st_size
            results['successful'] += 1
            results['size'] += file_size
            print(f"    âœ… æˆåŠŸ: {file_size:,} bytes")
        else:
            print(f"    âŒ å¤±è´¥")
    
    return results


def get_neurips_papers_batch(session, batch_id: int, total_batches: int = 6):
    """è·å–æŒ‡å®šæ‰¹æ¬¡çš„NeurIPSè®ºæ–‡"""
    # å…ˆè·å–æ€»æ•°
    count_query = text("""
        SELECT COUNT(*)
        FROM paper_interpretations pi
        JOIN papers p ON pi.paper_id = p.id
        LEFT JOIN paper_translations pt ON pi.paper_id = pt.paper_id
        WHERE pi.interpretation IS NOT NULL 
        AND LENGTH(pi.interpretation) > 100
        AND p.source = 'conf/neurips2025'
    """)
    
    total_count = session.execute(count_query).scalar()
    papers_per_batch = (total_count + total_batches - 1) // total_batches
    offset = batch_id * papers_per_batch
    
    query = text("""
        SELECT 
            pi.paper_id, 
            p.title,
            COALESCE(pt.title_zh, p.title) as title_zh,
            pi.interpretation
        FROM paper_interpretations pi
        JOIN papers p ON pi.paper_id = p.id
        LEFT JOIN paper_translations pt ON pi.paper_id = pt.paper_id
        WHERE pi.interpretation IS NOT NULL 
        AND LENGTH(pi.interpretation) > 100
        AND p.source = 'conf/neurips2025'
        ORDER BY pi.paper_id
        LIMIT :limit OFFSET :offset
    """)
    
    result = session.execute(query, {"limit": papers_per_batch, "offset": offset})
    papers = [(
        row[0] if isinstance(row[0], UUID) else UUID(row[0]), 
        row[1], row[2], row[3]
    ) for row in result.fetchall()]
    
    return papers, total_count


async def main():
    parser = argparse.ArgumentParser(description="æ–¹æ¡ˆ2ï¼šNeurIPSè®ºæ–‡åˆ†æ®µTTSç”Ÿæˆ")
    parser.add_argument("--batch-id", type=int, required=True, help="æ‰¹æ¬¡ID (0-5)")
    parser.add_argument("--concurrency", type=int, default=12, help="å¹¶å‘æ•°")
    parser.add_argument("--voice", default="zh-CN-XiaoxiaoNeural", help="è¯­éŸ³æ¨¡å‹")
    args = parser.parse_args()
    
    if args.batch_id < 0 or args.batch_id > 5:
        print("âŒ æ‰¹æ¬¡IDå¿…é¡»åœ¨0-5ä¹‹é—´")
        return
    
    print(f"ğŸµ æ–¹æ¡ˆ2 - æ‰¹æ¬¡{args.batch_id+1}/6ï¼Œå¹¶å‘:{args.concurrency}")
    
    output_dir = Path("backend/data/tts")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    db = SessionLocal()
    
    try:
        papers, total_count = get_neurips_papers_batch(db, args.batch_id, total_batches=6)
        
        if not papers:
            print("âŒ å½“å‰æ‰¹æ¬¡æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡")
            return
        
        print(f"ğŸ“š æ‰¹æ¬¡{args.batch_id+1}: {len(papers)}ç¯‡è®ºæ–‡ (æ€»è®¡:{total_count}ç¯‡)")
        
        total_stats = {'processed': 0, 'successful_segments': 0, 'total_segments': 0, 'total_size': 0}
        
        semaphore = asyncio.Semaphore(args.concurrency)
        
        async def process_with_semaphore(paper_data):
            async with semaphore:
                paper_id, title_en, title_zh, interpretation = paper_data
                try:
                    return await process_paper(paper_id, title_en, title_zh, interpretation, output_dir, args.voice)
                except Exception as e:
                    print(f"âŒ å¤„ç†è®ºæ–‡ {paper_id} å¤±è´¥: {e}")
                    return {'successful': 0, 'total': 0, 'size': 0}
        
        tasks = [process_with_semaphore(paper_data) for paper_data in papers]
        results = await asyncio.gather(*tasks)
        
        for result in results:
            total_stats['processed'] += 1
            total_stats['successful_segments'] += result['successful']
            total_stats['total_segments'] += result['total']
            total_stats['total_size'] += result['size']
        
        print(f"\nğŸ‰ æ‰¹æ¬¡{args.batch_id+1}å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡:")
        print(f"  å¤„ç†è®ºæ–‡: {total_stats['processed']}")
        print(f"  æˆåŠŸç‰‡æ®µ: {total_stats['successful_segments']}/{total_stats['total_segments']}")
        if total_stats['total_segments'] > 0:
            print(f"  æˆåŠŸç‡: {total_stats['successful_segments']/total_stats['total_segments']*100:.1f}%")
        print(f"  æ€»å¤§å°: {total_stats['total_size']:,} bytes ({total_stats['total_size']/1024/1024:.1f} MB)")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


if __name__ == "__main__":
    asyncio.run(main())
