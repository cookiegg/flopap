#!/usr/bin/env python3
"""
ä¼šè®®è®ºæ–‡å¤„ç†æµæ°´çº¿
1. ä¼šè®®è®ºæ–‡æ•°æ®è·å–
2. è´¨é‡ç­›é€‰å’Œåˆ†ç±»
3. æ‰¹é‡å†…å®¹ç”Ÿæˆ
4. ä¼šè®®æ¨èæ± ç”Ÿæˆ
5. ä¸“é¢˜æ¨èåˆ›å»º
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from datetime import datetime, date
from app.db.session import SessionLocal
from sqlalchemy import text

def step1_conference_data_ingestion():
    """æ­¥éª¤1: ä¼šè®®è®ºæ–‡æ•°æ®è·å–"""
    print("ğŸ”„ æ­¥éª¤1: ä¼šè®®è®ºæ–‡æ•°æ®è·å–")
    
    # è¿™é‡Œå¯ä»¥é›†æˆå„ç§ä¼šè®®æ•°æ®æº
    # ä¾‹å¦‚: NIPS, ICML, ICLR, ACL, EMNLPç­‰
    
    conference_sources = [
        'NIPS', 'ICML', 'ICLR', 'ACL', 'EMNLP', 
        'CVPR', 'ICCV', 'ECCV', 'AAAI', 'IJCAI'
    ]
    
    with SessionLocal() as db:
        # ç»Ÿè®¡ç°æœ‰ä¼šè®®è®ºæ–‡
        existing_conference_papers = db.execute(text("""
            SELECT 
                COUNT(*) as total,
                COUNT(*) FILTER (WHERE created_at >= CURRENT_DATE - INTERVAL '30 days') as recent
            FROM papers 
            WHERE comment ILIKE ANY(ARRAY['%NIPS%', '%ICML%', '%ICLR%', '%ACL%', '%EMNLP%', 
                                          '%CVPR%', '%ICCV%', '%ECCV%', '%AAAI%', '%IJCAI%'])
        """)).fetchone()
        
        print(f"  ğŸ“Š ç°æœ‰ä¼šè®®è®ºæ–‡: {existing_conference_papers[0]}ç¯‡")
        print(f"  ğŸ“… è¿‘30å¤©æ–°å¢: {existing_conference_papers[1]}ç¯‡")
        
        # æ¨¡æ‹Ÿæ–°ä¼šè®®è®ºæ–‡è·å–
        # å®é™…å®ç°ä¸­è¿™é‡Œä¼šè°ƒç”¨å„ä¼šè®®çš„APIæˆ–çˆ¬è™«
        print(f"  ğŸ” æ‰«æä¼šè®®æº: {', '.join(conference_sources)}")
        print(f"  ğŸ“¥ æ¨¡æ‹Ÿè·å–æ–°è®ºæ–‡: å‡è®¾è·å–äº†25ç¯‡æ–°ä¼šè®®è®ºæ–‡")
        
        return 25  # æ¨¡æ‹Ÿè¿”å›å€¼

def step2_quality_filtering_and_classification():
    """æ­¥éª¤2: è´¨é‡ç­›é€‰å’Œåˆ†ç±»"""
    print("\nğŸ”„ æ­¥éª¤2: è´¨é‡ç­›é€‰å’Œåˆ†ç±»")
    
    with SessionLocal() as db:
        # è·å–æœ€è¿‘çš„ä¼šè®®è®ºæ–‡
        conference_papers = db.execute(text("""
            SELECT id, title, summary, comment, categories
            FROM papers 
            WHERE comment ILIKE ANY(ARRAY['%NIPS%', '%ICML%', '%ICLR%', '%ACL%', '%EMNLP%', 
                                          '%CVPR%', '%ICCV%', '%ECCV%', '%AAAI%', '%IJCAI%'])
            AND created_at >= CURRENT_DATE - INTERVAL '7 days'
            ORDER BY created_at DESC
        """)).fetchall()
        
        print(f"  ğŸ“š å¾…å¤„ç†ä¼šè®®è®ºæ–‡: {len(conference_papers)}ç¯‡")
        
        # è´¨é‡ç­›é€‰è§„åˆ™
        high_quality_papers = []
        categorized_papers = {
            'AI/ML': [],
            'CV': [],
            'NLP': [],
            'Other': []
        }
        
        for paper in conference_papers:
            paper_id, title, summary, comment, categories = paper
            
            # è´¨é‡ç­›é€‰ (åŸºäºæ ‡é¢˜é•¿åº¦ã€æ‘˜è¦é•¿åº¦ç­‰ç®€å•è§„åˆ™)
            if (len(title) >= 10 and len(summary) >= 100 and 
                not any(word in title.lower() for word in ['test', 'demo', 'workshop'])):
                high_quality_papers.append(paper_id)
                
                # åˆ†ç±»
                if any(cat.startswith('cs.CV') for cat in (categories or [])):
                    categorized_papers['CV'].append(paper_id)
                elif any(cat.startswith('cs.CL') for cat in (categories or [])):
                    categorized_papers['NLP'].append(paper_id)
                elif any(cat.startswith(('cs.AI', 'cs.LG')) for cat in (categories or [])):
                    categorized_papers['AI/ML'].append(paper_id)
                else:
                    categorized_papers['Other'].append(paper_id)
        
        print(f"  âœ… é«˜è´¨é‡è®ºæ–‡: {len(high_quality_papers)}ç¯‡")
        print(f"  ğŸ“Š åˆ†ç±»ç»“æœ:")
        for category, papers in categorized_papers.items():
            print(f"    - {category}: {len(papers)}ç¯‡")
        
        return high_quality_papers, categorized_papers

def step3_batch_content_generation(paper_ids):
    """æ­¥éª¤3: æ‰¹é‡å†…å®¹ç”Ÿæˆ"""
    print(f"\nğŸ”„ æ­¥éª¤3: æ‰¹é‡å†…å®¹ç”Ÿæˆ ({len(paper_ids)}ç¯‡è®ºæ–‡)")
    
    from app.services.translation_pure import batch_translate_papers
    from app.services.ai_interpretation_pure import interpret_and_save_papers
    
    with SessionLocal() as db:
        # æ‰¹é‡ç¿»è¯‘
        print("  ğŸ”¤ å¼€å§‹æ‰¹é‡ç¿»è¯‘...")
        translated_count = batch_translate_papers(db, paper_ids)
        print(f"    âœ… ç¿»è¯‘å®Œæˆ: {translated_count}ç¯‡")
        
        # æ‰¹é‡AIè§£è¯»
        print("  ğŸ¤– å¼€å§‹æ‰¹é‡AIè§£è¯»...")
        interpreted_count = interpret_and_save_papers(db, paper_ids)
        print(f"    âœ… AIè§£è¯»å®Œæˆ: {interpreted_count}ç¯‡")
        
        # ç”Ÿæˆä¿¡æ¯å›¾ (å¦‚æœéœ€è¦)
        print("  ğŸ“Š ç”Ÿæˆä¿¡æ¯å›¾...")
        # è¿™é‡Œå¯ä»¥è°ƒç”¨ä¿¡æ¯å›¾ç”ŸæˆæœåŠ¡
        infographic_count = min(len(paper_ids), 10)  # æ¨¡æ‹Ÿåªä¸ºå‰10ç¯‡ç”Ÿæˆä¿¡æ¯å›¾
        print(f"    âœ… ä¿¡æ¯å›¾ç”Ÿæˆ: {infographic_count}ç¯‡")
        
        return translated_count, interpreted_count, infographic_count

def step4_conference_recommendation_pool(categorized_papers):
    """æ­¥éª¤4: ä¼šè®®æ¨èæ± ç”Ÿæˆ"""
    print("\nğŸ”„ æ­¥éª¤4: ä¼šè®®æ¨èæ± ç”Ÿæˆ")
    
    with SessionLocal() as db:
        today = date.today()
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºæ¨èæ± 
        total_pools = 0
        
        for category, paper_ids in categorized_papers.items():
            if not paper_ids:
                continue
            
            # æ¸…ç†è¯¥ç±»åˆ«çš„æ—§æ¨èæ± 
            db.execute(text("""
                DELETE FROM conference_recommendation_pool 
                WHERE source = :category AND pool_date = :date
            """), {'category': category, 'date': today})
            
            # åˆ›å»ºæ–°çš„æ¨èæ± 
            for position, paper_id in enumerate(paper_ids[:20]):  # æ¯ç±»åˆ«æœ€å¤š20ç¯‡
                db.execute(text("""
                    INSERT INTO conference_recommendation_pool 
                    (pool_date, paper_id, source, position, score, is_active, created_at, updated_at)
                    VALUES (:date, :paper_id, :source, :position, :score, true, NOW(), NOW())
                """), {
                    'date': today,
                    'paper_id': paper_id,
                    'source': category,
                    'position': position,
                    'score': 1.0 - (position * 0.01)  # ç®€å•çš„ä½ç½®è¯„åˆ†
                })
            
            total_pools += len(paper_ids[:20])
            print(f"    âœ… {category}æ¨èæ± : {len(paper_ids[:20])}ç¯‡")
        
        db.commit()
        print(f"  âœ… ä¼šè®®æ¨èæ± ç”Ÿæˆ: {total_pools}ç¯‡è®ºæ–‡")
        
        return total_pools

def step5_create_special_recommendations():
    """æ­¥éª¤5: åˆ›å»ºä¸“é¢˜æ¨è"""
    print("\nğŸ”„ æ­¥éª¤5: åˆ›å»ºä¸“é¢˜æ¨è")
    
    with SessionLocal() as db:
        # åˆ›å»º"æœ¬å‘¨çƒ­é—¨ä¼šè®®è®ºæ–‡"ä¸“é¢˜
        hot_papers = db.execute(text("""
            SELECT p.id, COUNT(uf.id) as feedback_count
            FROM papers p
            LEFT JOIN user_feedback uf ON p.id = uf.paper_id AND uf.feedback_type = 'like'
            WHERE p.comment ILIKE ANY(ARRAY['%NIPS%', '%ICML%', '%ICLR%', '%ACL%', '%EMNLP%', 
                                            '%CVPR%', '%ICCV%', '%ECCV%', '%AAAI%', '%IJCAI%'])
            AND p.created_at >= CURRENT_DATE - INTERVAL '7 days'
            GROUP BY p.id
            ORDER BY feedback_count DESC, p.created_at DESC
            LIMIT 10
        """)).fetchall()
        
        print(f"    ğŸ“ˆ æœ¬å‘¨çƒ­é—¨ä¼šè®®è®ºæ–‡: {len(hot_papers)}ç¯‡")
        
        # åˆ›å»º"æ–°å…´æŠ€æœ¯è¶‹åŠ¿"ä¸“é¢˜
        trending_keywords = ['transformer', 'diffusion', 'multimodal', 'few-shot', 'self-supervised']
        trending_papers = []
        
        for keyword in trending_keywords:
            papers = db.execute(text("""
                SELECT id FROM papers
                WHERE (title ILIKE :keyword OR summary ILIKE :keyword)
                AND comment ILIKE ANY(ARRAY['%NIPS%', '%ICML%', '%ICLR%', '%ACL%', '%EMNLP%', 
                                            '%CVPR%', '%ICCV%', '%ECCV%', '%AAAI%', '%IJCAI%'])
                AND created_at >= CURRENT_DATE - INTERVAL '30 days'
                ORDER BY created_at DESC
                LIMIT 2
            """), {'keyword': f'%{keyword}%'}).fetchall()
            
            trending_papers.extend([p[0] for p in papers])
        
        print(f"    ğŸ”¥ æ–°å…´æŠ€æœ¯è¶‹åŠ¿è®ºæ–‡: {len(trending_papers)}ç¯‡")
        
        # åˆ›å»º"è·¨é¢†åŸŸç ”ç©¶"ä¸“é¢˜
        interdisciplinary_papers = db.execute(text("""
            SELECT id FROM papers
            WHERE array_length(categories, 1) >= 3
            AND comment ILIKE ANY(ARRAY['%NIPS%', '%ICML%', '%ICLR%', '%ACL%', '%EMNLP%', 
                                        '%CVPR%', '%ICCV%', '%ECCV%', '%AAAI%', '%IJCAI%'])
            AND created_at >= CURRENT_DATE - INTERVAL '14 days'
            ORDER BY array_length(categories, 1) DESC, created_at DESC
            LIMIT 8
        """)).fetchall()
        
        print(f"    ğŸ”— è·¨é¢†åŸŸç ”ç©¶è®ºæ–‡: {len(interdisciplinary_papers)}ç¯‡")
        
        # ä¿å­˜ä¸“é¢˜æ¨èåˆ°ç‰¹æ®Šæ ‡è®°
        special_recommendations = {
            'weekly_hot': [p[0] for p in hot_papers],
            'trending_tech': trending_papers,
            'interdisciplinary': [p[0] for p in interdisciplinary_papers]
        }
        
        return special_recommendations

def main():
    """ä¸»æµç¨‹"""
    print("ğŸš€ å¼€å§‹ä¼šè®®è®ºæ–‡å¤„ç†æµæ°´çº¿")
    start_time = datetime.now()
    
    try:
        # æ­¥éª¤1: ä¼šè®®æ•°æ®è·å–
        new_papers_count = step1_conference_data_ingestion()
        
        # æ­¥éª¤2: è´¨é‡ç­›é€‰å’Œåˆ†ç±»
        high_quality_papers, categorized_papers = step2_quality_filtering_and_classification()
        
        if not high_quality_papers:
            print("âš ï¸  æ— é«˜è´¨é‡ä¼šè®®è®ºæ–‡éœ€è¦å¤„ç†")
            return
        
        # æ­¥éª¤3: æ‰¹é‡å†…å®¹ç”Ÿæˆ
        translated, interpreted, infographics = step3_batch_content_generation(high_quality_papers)
        
        # æ­¥éª¤4: ä¼šè®®æ¨èæ± ç”Ÿæˆ
        pool_count = step4_conference_recommendation_pool(categorized_papers)
        
        # æ­¥éª¤5: ä¸“é¢˜æ¨èåˆ›å»º
        special_recs = step5_create_special_recommendations()
        
        # æ€»ç»“
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nğŸ‰ ä¼šè®®è®ºæ–‡å¤„ç†æµæ°´çº¿å®Œæˆ!")
        print(f"â±ï¸  æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"   - æ–°è·å–è®ºæ–‡: {new_papers_count}ç¯‡")
        print(f"   - é«˜è´¨é‡è®ºæ–‡: {len(high_quality_papers)}ç¯‡")
        print(f"   - ç¿»è¯‘: {translated}ç¯‡")
        print(f"   - AIè§£è¯»: {interpreted}ç¯‡")
        print(f"   - ä¿¡æ¯å›¾: {infographics}ç¯‡")
        print(f"   - æ¨èæ± : {pool_count}ç¯‡")
        print(f"   - ä¸“é¢˜æ¨è: {sum(len(papers) for papers in special_recs.values())}ç¯‡")
        
        print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
        for category, papers in categorized_papers.items():
            print(f"   - {category}: {len(papers)}ç¯‡")
        
    except Exception as e:
        print(f"âŒ ä¼šè®®è®ºæ–‡æµæ°´çº¿å¼‚å¸¸: {e}")
        raise

if __name__ == "__main__":
    main()
