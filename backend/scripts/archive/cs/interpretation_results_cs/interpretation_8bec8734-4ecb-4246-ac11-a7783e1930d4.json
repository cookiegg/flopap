{
  "paper_id": "8bec8734-4ecb-4246-ac11-a7783e1930d4",
  "arxiv_id": "2512.12264v1",
  "title": "Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics",
  "ai_interpretation": "这篇论文提出了一个名为**Market-Bench**的新基准，专门用于评估大语言模型（LLM）在**入门级量化交易**任务上的能力。下面为你进行通俗解读：\n\n### 1. 研究背景与动机\n量化交易依赖数学模型和代码实现交易策略。随着LLM在代码生成上表现突出，一个自然的问题是：**LLM能否理解自然语言描述的金融策略，并生成可执行、结果正确的回测代码？** 目前缺乏专门评估LLM在此交叉领域能力的基准，本文旨在填补这一空白。\n\n### 2. 核心创新点与贡献\n- **首创性基准**：首次构建了一个专注于“从策略描述生成量化回测代码”的评估框架。\n- **贴近实战**：要求模型生成的代码不仅能运行，其输出的**盈亏、回撤、持仓路径**等关键指标还必须与参考答案高度吻合。\n- **公开平台**：发布了公开的基准测试网站和排行榜，推动该领域透明化研究。\n\n### 3. 技术方法详解\n- **任务设计**：包含三个经典策略任务：①微软定时交易、②可口可乐与百事配对交易、③微软Delta对冲（期权风险管理）。每个任务都给出了自然语言描述和市场假设。\n- **评估指标**：采用**两阶段评估法**：\n  1. **结构可靠性**：生成的代码能否无错运行？（用pass@k衡量）\n  2. **数值准确性**：运行结果的关键指标与参考答案的误差有多大？（用平均绝对误差衡量）\n- **测试模型**：评估了包括GPT、Claude、Gemini、Qwen等在内的12个前沿模型。\n\n### 4. 实验结果分析\n- **模型表现分化明显**：\n  - **GPT-5.1 Codex-Max**在最简单任务上表现完美，误差最低。\n  - **Gemini 3 Pro**和**Claude 4.5 Sonnet**在简单策略上兼具高可靠性与低误差。\n  - **Qwen3 Max**代码总能运行（高可靠性），但有时生成错误的盈亏路径（数值准确性不足）。\n- **核心发现**：当前LLM能为基础交易策略搭建代码框架，但在**精确推理价格、库存和风险**等复杂金融概念时仍存在显著困难。模型表现严重依赖于任务复杂度。\n\n### 5. 学术价值与影响\n- 为**AI+金融**交叉研究提供了首个聚焦代码生成与金融逻辑推理的严谨评估工具。\n- 揭示了当前LLM在需要精确数值计算和连续状态推理的领域存在局限，推动了面向**可靠性与准确性**的模型能力研究。\n- 其两阶段评估方法论（结构 vs 数值）对评估代码生成模型在其他专业领域（如科学计算、工程仿真）的应用具有借鉴意义。\n\n### 6. 未来研究方向\n- 开发更复杂的策略任务（如多资产组合、高频交易）。\n- 探索让LLM更好地理解和推理**时间序列数据**和**动态风险**的方法。\n- 研究如何将领域知识（如金融约束）更有效地融入模型训练或提示工程中，以提高输出结果的金融合理性。\n\n**总结**：Market-Bench像一场“量化交易编程考试”，发现顶尖LLM已是“及格的程序员”，能搭建代码骨架，但还不是“优秀的量化分析师”，在金融逻辑的精确性上仍需加强。这项工作为衡量和提升AI在金融工程领域的实用化能力奠定了基础。",
  "timestamp": "2025-12-16T22:32:47.312020",
  "model_name": "deepseek-reasoner"
}