#!/usr/bin/env python3
"""
å€™é€‰æ± ç¿»è¯‘è„šæœ¬
åŠŸèƒ½ï¼šå¯¹ç­›é€‰åçš„å€™é€‰æ± è®ºæ–‡è¿›è¡Œæ‰¹é‡ç¿»è¯‘
ç‰¹ç‚¹ï¼š
1. å…ˆä¿å­˜ç¿»è¯‘ç»“æœåˆ°æ–‡ä»¶ï¼Œå†è½¬å‚¨åˆ°æ•°æ®åº“
2. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé¿å…é‡å¤ç¿»è¯‘
3. å……åˆ†åˆ©ç”¨50ä¸ªAPI KEYå¹¶å‘å¤„ç†
4. é”™è¯¯éš”ç¦»ï¼Œå•ç¯‡å¤±è´¥ä¸å½±å“æ•´ä½“è¿›åº¦
"""

import json
import os
import sys
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.db.session import SessionLocal
from app.services.candidate_pool import CandidatePoolService
from app.services.translation_pure import translate_single_paper
from app.services.llm import get_deepseek_clients, distribute_papers
from app.models import Paper, PaperTranslation
from app.core.config import settings
from sqlalchemy import select
from loguru import logger


def translate_papers_to_files(
    papers: List[Paper], 
    output_dir: str, 
    max_workers: int = 50
) -> Dict[str, int]:
    """
    ç¿»è¯‘è®ºæ–‡å¹¶ä¿å­˜åˆ°JSONæ–‡ä»¶
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        max_workers: æœ€å¤§å¹¶å‘æ•°
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    papers_to_translate = []
    for paper in papers:
        filename = f"{output_dir}/translation_{paper.id}.json"
        if not os.path.exists(filename):
            papers_to_translate.append(paper)
    
    logger.info(f"éœ€è¦ç¿»è¯‘ {len(papers_to_translate)} ç¯‡è®ºæ–‡ï¼ˆè·³è¿‡å·²å­˜åœ¨æ–‡ä»¶ {len(papers) - len(papers_to_translate)} ç¯‡ï¼‰")
    
    if not papers_to_translate:
        return {"success": 0, "failed": 0, "skipped": len(papers)}
    
    # è·å–DeepSeekå®¢æˆ·ç«¯æ± 
    clients = get_deepseek_clients()
    paper_groups = distribute_papers(papers_to_translate, len(clients))
    
    success_count = 0
    failed_count = 0
    
    # å¹¶å‘ç¿»è¯‘
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_paper = {}
        
        for client, paper_group in zip(clients, paper_groups):
            for paper in paper_group:
                future = executor.submit(translate_single_paper, client, paper)
                future_to_paper[future] = paper
        
        for future in as_completed(future_to_paper):
            paper = future_to_paper[future]
            try:
                result = future.result()
                if result:
                    title_zh, summary_zh = result
                    
                    # æ„å»ºç¿»è¯‘æ•°æ®
                    translation_data = {
                        'paper_id': str(paper.id),
                        'arxiv_id': paper.arxiv_id,
                        'title_en': paper.title,
                        'title_zh': title_zh,
                        'summary_en': paper.summary,
                        'summary_zh': summary_zh,
                        'timestamp': datetime.now().isoformat(),
                        'model_name': settings.deepseek_model_name or 'deepseek-chat'
                    }
                    
                    # ä¿å­˜åˆ°JSONæ–‡ä»¶
                    filename = f"{output_dir}/translation_{paper.id}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(translation_data, f, ensure_ascii=False, indent=2)
                    
                    success_count += 1
                    if success_count % 20 == 0:
                        logger.info(f"å·²å®Œæˆ {success_count}/{len(papers_to_translate)} ç¯‡ç¿»è¯‘")
                else:
                    failed_count += 1
                    logger.error(f"ç¿»è¯‘è®ºæ–‡ {paper.id} å¤±è´¥")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"ç¿»è¯‘è®ºæ–‡ {paper.id} å¼‚å¸¸: {e}")
    
    return {"success": success_count, "failed": failed_count, "skipped": len(papers) - len(papers_to_translate)}


def load_translations_to_database(output_dir: str) -> Dict[str, int]:
    """
    ä»JSONæ–‡ä»¶æ‰¹é‡åŠ è½½ç¿»è¯‘ç»“æœåˆ°æ•°æ®åº“
    
    Args:
        output_dir: ç¿»è¯‘æ–‡ä»¶ç›®å½•
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    if not os.path.exists(output_dir):
        logger.error(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return {"success": 0, "failed": 0, "skipped": 0}
    
    json_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]
    logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªç¿»è¯‘æ–‡ä»¶")
    
    success_count = 0
    failed_count = 0
    skipped_count = 0
    
    with SessionLocal() as session:
        for filename in json_files:
            try:
                filepath = os.path.join(output_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                paper_id = data['paper_id']
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å®Œæ•´ç¿»è¯‘
                existing = session.execute(
                    select(PaperTranslation).where(PaperTranslation.paper_id == paper_id)
                ).scalar_one_or_none()
                
                if existing and existing.title_zh and existing.summary_zh:
                    skipped_count += 1
                    continue
                
                # åˆ›å»ºæˆ–æ›´æ–°ç¿»è¯‘è®°å½•
                if existing:
                    existing.title_zh = data['title_zh']
                    existing.summary_zh = data['summary_zh']
                    existing.model_name = data.get('model_name', 'deepseek-chat')
                else:
                    translation = PaperTranslation(
                        paper_id=paper_id,
                        title_zh=data['title_zh'],
                        summary_zh=data['summary_zh'],
                        model_name=data.get('model_name', 'deepseek-chat')
                    )
                    session.add(translation)
                
                success_count += 1
                
                # æ‰¹é‡æäº¤ï¼Œæé«˜æ€§èƒ½
                if success_count % 50 == 0:
                    session.commit()
                    logger.info(f"å·²ä¿å­˜ {success_count} æ¡ç¿»è¯‘è®°å½•åˆ°æ•°æ®åº“")
                
            except Exception as e:
                failed_count += 1
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        # æœ€ç»ˆæäº¤
        session.commit()
    
    return {"success": success_count, "failed": failed_count, "skipped": skipped_count}


def get_translation_status(batch_id: str, filter_type: str) -> Dict[str, int]:
    """
    è·å–å€™é€‰æ± ç¿»è¯‘çŠ¶æ€
    
    Args:
        batch_id: æ‰¹æ¬¡ID
        filter_type: ç­›é€‰ç±»å‹
        
    Returns:
        ç¿»è¯‘çŠ¶æ€ç»Ÿè®¡
    """
    with SessionLocal() as session:
        # è·å–å€™é€‰æ± è®ºæ–‡ID
        candidate_paper_ids = CandidatePoolService.get_candidate_paper_ids(
            session, batch_id, filter_type
        )
        
        # æ£€æŸ¥ç¿»è¯‘çŠ¶æ€
        translated_count = 0
        for paper_id in candidate_paper_ids:
            translation = session.execute(
                select(PaperTranslation).where(PaperTranslation.paper_id == paper_id)
            ).scalar_one_or_none()
            
            if translation and translation.title_zh and translation.summary_zh:
                translated_count += 1
        
        return {
            "total": len(candidate_paper_ids),
            "translated": translated_count,
            "remaining": len(candidate_paper_ids) - translated_count
        }


def main():
    parser = argparse.ArgumentParser(description="å€™é€‰æ± è®ºæ–‡ç¿»è¯‘è„šæœ¬")
    parser.add_argument('batch_id', help='æ‰¹æ¬¡ID')
    parser.add_argument('filter_type', help='ç­›é€‰ç±»å‹ (cs, ai-ml-cv, math, physics, all)')
    parser.add_argument('--max-workers', type=int, default=50, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 50)')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½• (é»˜è®¤: translation_results_<filter_type>)')
    parser.add_argument('--only-translate', action='store_true', help='åªç¿»è¯‘åˆ°æ–‡ä»¶ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“')
    parser.add_argument('--only-load', action='store_true', help='åªä»æ–‡ä»¶åŠ è½½åˆ°æ•°æ®åº“')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹ç¿»è¯‘çŠ¶æ€')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    if not args.output_dir:
        args.output_dir = f"translation_results_{args.filter_type}"
    
    # æŸ¥çœ‹çŠ¶æ€
    if args.status:
        status = get_translation_status(args.batch_id, args.filter_type)
        print(f"\nğŸ“Š å€™é€‰æ± ç¿»è¯‘çŠ¶æ€:")
        print(f"  æ‰¹æ¬¡ID: {args.batch_id}")
        print(f"  ç­›é€‰ç±»å‹: {args.filter_type}")
        print(f"  æ€»è®ºæ–‡æ•°: {status['total']} ç¯‡")
        print(f"  å·²ç¿»è¯‘: {status['translated']} ç¯‡")
        print(f"  æœªç¿»è¯‘: {status['remaining']} ç¯‡")
        print(f"  å®Œæˆç‡: {status['translated']/status['total']*100:.1f}%")
        return
    
    # åªåŠ è½½æ–‡ä»¶åˆ°æ•°æ®åº“
    if args.only_load:
        logger.info("ä»æ–‡ä»¶åŠ è½½ç¿»è¯‘ç»“æœåˆ°æ•°æ®åº“...")
        result = load_translations_to_database(args.output_dir)
        logger.success(f"æ•°æ®åº“åŠ è½½å®Œæˆ: æˆåŠŸ {result['success']}, å¤±è´¥ {result['failed']}, è·³è¿‡ {result['skipped']}")
        return
    
    # ç¿»è¯‘é˜¶æ®µ
    with SessionLocal() as session:
        papers = CandidatePoolService.get_candidate_papers(session, args.batch_id, args.filter_type)
        logger.info(f"å€™é€‰æ± åŒ…å« {len(papers)} ç¯‡ {args.filter_type} è®ºæ–‡")
    
    if not papers:
        logger.warning("å€™é€‰æ± ä¸ºç©ºï¼Œè¯·å…ˆåˆ›å»ºå€™é€‰æ± ")
        return
    
    logger.info(f"å¼€å§‹ç¿»è¯‘å¹¶ä¿å­˜åˆ°æ–‡ä»¶ ({args.output_dir})...")
    translate_result = translate_papers_to_files(papers, args.output_dir, args.max_workers)
    logger.success(f"ç¿»è¯‘å®Œæˆ: æˆåŠŸ {translate_result['success']}, å¤±è´¥ {translate_result['failed']}, è·³è¿‡ {translate_result['skipped']}")
    
    # è‡ªåŠ¨åŠ è½½åˆ°æ•°æ®åº“ï¼ˆé™¤éæŒ‡å®šåªç¿»è¯‘ï¼‰
    if not args.only_translate:
        logger.info("åŠ è½½ç¿»è¯‘ç»“æœåˆ°æ•°æ®åº“...")
        load_result = load_translations_to_database(args.output_dir)
        logger.success(f"æ•°æ®åº“åŠ è½½å®Œæˆ: æˆåŠŸ {load_result['success']}, å¤±è´¥ {load_result['failed']}, è·³è¿‡ {load_result['skipped']}")
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        final_status = get_translation_status(args.batch_id, args.filter_type)
        print(f"\nğŸ‰ ç¿»è¯‘ä»»åŠ¡å®Œæˆ!")
        print(f"  å€™é€‰æ± : {final_status['total']} ç¯‡ {args.filter_type} è®ºæ–‡")
        print(f"  ç¿»è¯‘å®Œæˆ: {final_status['translated']} ç¯‡ ({final_status['translated']/final_status['total']*100:.1f}%)")


if __name__ == "__main__":
    main()
