#!/usr/bin/env python3
"""
å€™é€‰æ± AIè§£è¯»ç”Ÿæˆè„šæœ¬
åŠŸèƒ½ï¼šå¯¹ç­›é€‰åçš„å€™é€‰æ± è®ºæ–‡è¿›è¡Œæ‰¹é‡AIè§£è¯»ç”Ÿæˆ
ç‰¹ç‚¹ï¼š
1. å…ˆä¿å­˜AIè§£è¯»ç»“æœåˆ°æ–‡ä»¶ï¼Œå†è½¬å‚¨åˆ°æ•°æ®åº“
2. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé¿å…é‡å¤ç”Ÿæˆ
3. å……åˆ†åˆ©ç”¨50ä¸ªAPI KEYå¹¶å‘å¤„ç†
4. é”™è¯¯éš”ç¦»ï¼Œå•ç¯‡å¤±è´¥ä¸å½±å“æ•´ä½“è¿›åº¦
"""

import json
import os
import sys
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.db.session import SessionLocal
from app.services.candidate_pool import CandidatePoolService
from app.services.ai_interpretation import generate_ai_interpretation
from app.services.llm import get_deepseek_clients, distribute_papers
from app.models import Paper
from app.models.paper import PaperInterpretation
from app.core.config import settings
from sqlalchemy import select
from loguru import logger


def generate_interpretations_to_files(
    papers: List[Paper], 
    output_dir: str, 
    max_workers: int = 50
) -> Dict[str, int]:
    """
    ç”ŸæˆAIè§£è¯»å¹¶ä¿å­˜åˆ°JSONæ–‡ä»¶
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        max_workers: æœ€å¤§å¹¶å‘æ•°
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    papers_to_interpret = []
    for paper in papers:
        filename = f"{output_dir}/interpretation_{paper.id}.json"
        if not os.path.exists(filename):
            papers_to_interpret.append(paper)
    
    logger.info(f"éœ€è¦ç”ŸæˆAIè§£è¯» {len(papers_to_interpret)} ç¯‡è®ºæ–‡ï¼ˆè·³è¿‡å·²å­˜åœ¨æ–‡ä»¶ {len(papers) - len(papers_to_interpret)} ç¯‡ï¼‰")
    
    if not papers_to_interpret:
        return {"success": 0, "failed": 0, "skipped": len(papers)}
    
    # è·å–DeepSeekå®¢æˆ·ç«¯æ± 
    clients = get_deepseek_clients()
    paper_groups = distribute_papers(papers_to_interpret, len(clients))
    
    success_count = 0
    failed_count = 0
    
    # å¹¶å‘ç”ŸæˆAIè§£è¯»
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_paper = {}
        
        for client, paper_group in zip(clients, paper_groups):
            for paper in paper_group:
                future = executor.submit(generate_ai_interpretation, client, paper)
                future_to_paper[future] = paper
        
        for future in as_completed(future_to_paper):
            paper = future_to_paper[future]
            try:
                ai_interpretation = future.result()
                if ai_interpretation and ai_interpretation.strip():
                    
                    # æ„å»ºAIè§£è¯»æ•°æ®
                    interpretation_data = {
                        'paper_id': str(paper.id),
                        'arxiv_id': paper.arxiv_id,
                        'title': paper.title,
                        'ai_interpretation': ai_interpretation,
                        'timestamp': datetime.now().isoformat(),
                        'model_name': settings.deepseek_model_name or 'deepseek-chat'
                    }
                    
                    # ä¿å­˜åˆ°JSONæ–‡ä»¶
                    filename = f"{output_dir}/interpretation_{paper.id}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(interpretation_data, f, ensure_ascii=False, indent=2)
                    
                    success_count += 1
                    if success_count % 20 == 0:
                        logger.info(f"å·²å®Œæˆ {success_count}/{len(papers_to_interpret)} ç¯‡AIè§£è¯»")
                else:
                    failed_count += 1
                    logger.error(f"ç”Ÿæˆè®ºæ–‡ {paper.id} AIè§£è¯»å¤±è´¥")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"ç”Ÿæˆè®ºæ–‡ {paper.id} AIè§£è¯»å¼‚å¸¸: {e}")
    
    return {"success": success_count, "failed": failed_count, "skipped": len(papers) - len(papers_to_interpret)}


def load_interpretations_to_database(output_dir: str) -> Dict[str, int]:
    """
    ä»JSONæ–‡ä»¶æ‰¹é‡åŠ è½½AIè§£è¯»ç»“æœåˆ°æ•°æ®åº“
    
    Args:
        output_dir: AIè§£è¯»æ–‡ä»¶ç›®å½•
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    if not os.path.exists(output_dir):
        logger.error(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return {"success": 0, "failed": 0, "skipped": 0}
    
    json_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]
    logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªAIè§£è¯»æ–‡ä»¶")
    
    success_count = 0
    failed_count = 0
    skipped_count = 0
    
    with SessionLocal() as session:
        for filename in json_files:
            try:
                filepath = os.path.join(output_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                paper_id = data['paper_id']
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨AIè§£è¯»
                existing = session.execute(
                    select(PaperInterpretation).where(PaperInterpretation.paper_id == paper_id)
                ).scalar_one_or_none()
                
                if existing and existing.interpretation and existing.interpretation.strip():
                    skipped_count += 1
                    continue
                
                # åˆ›å»ºæˆ–æ›´æ–°AIè§£è¯»è®°å½•
                if existing:
                    existing.interpretation = data['ai_interpretation']
                    if not existing.model_name:
                        existing.model_name = data.get('model_name', 'deepseek-chat')
                else:
                    interpretation = PaperInterpretation(
                        paper_id=paper_id,
                        interpretation=data['ai_interpretation'],
                        language='zh',
                        model_name=data.get('model_name', 'deepseek-chat')
                    )
                    session.add(interpretation)
                
                success_count += 1
                
                # æ‰¹é‡æäº¤ï¼Œæé«˜æ€§èƒ½
                if success_count % 50 == 0:
                    session.commit()
                    logger.info(f"å·²ä¿å­˜ {success_count} æ¡AIè§£è¯»è®°å½•åˆ°æ•°æ®åº“")
                
            except Exception as e:
                failed_count += 1
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        # æœ€ç»ˆæäº¤
        session.commit()
    
    return {"success": success_count, "failed": failed_count, "skipped": skipped_count}


def get_interpretation_status(batch_id: str, filter_type: str) -> Dict[str, int]:
    """
    è·å–å€™é€‰æ± AIè§£è¯»çŠ¶æ€
    
    Args:
        batch_id: æ‰¹æ¬¡ID
        filter_type: ç­›é€‰ç±»å‹
        
    Returns:
        AIè§£è¯»çŠ¶æ€ç»Ÿè®¡
    """
    with SessionLocal() as session:
        # è·å–å€™é€‰æ± è®ºæ–‡ID
        candidate_paper_ids = CandidatePoolService.get_candidate_paper_ids(
            session, batch_id, filter_type
        )
        
        # æ£€æŸ¥AIè§£è¯»çŠ¶æ€
        interpreted_count = 0
        for paper_id in candidate_paper_ids:
            translation = session.execute(
                select(PaperTranslation).where(PaperTranslation.paper_id == paper_id)
            ).scalar_one_or_none()
            
            if translation and translation.ai_interpretation and translation.ai_interpretation.strip():
                interpreted_count += 1
        
        return {
            "total": len(candidate_paper_ids),
            "interpreted": interpreted_count,
            "remaining": len(candidate_paper_ids) - interpreted_count
        }


def main():
    parser = argparse.ArgumentParser(description="å€™é€‰æ± è®ºæ–‡AIè§£è¯»ç”Ÿæˆè„šæœ¬")
    parser.add_argument('batch_id', help='æ‰¹æ¬¡ID')
    parser.add_argument('filter_type', help='ç­›é€‰ç±»å‹ (cs, ai-ml-cv, math, physics, all)')
    parser.add_argument('--max-workers', type=int, default=50, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 50)')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½• (é»˜è®¤: interpretation_results_<filter_type>)')
    parser.add_argument('--only-interpret', action='store_true', help='åªç”ŸæˆAIè§£è¯»åˆ°æ–‡ä»¶ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“')
    parser.add_argument('--only-load', action='store_true', help='åªä»æ–‡ä»¶åŠ è½½åˆ°æ•°æ®åº“')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹AIè§£è¯»çŠ¶æ€')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    if not args.output_dir:
        args.output_dir = f"interpretation_results_{args.filter_type}"
    
    # æŸ¥çœ‹çŠ¶æ€
    if args.status:
        status = get_interpretation_status(args.batch_id, args.filter_type)
        print(f"\nğŸ“Š å€™é€‰æ± AIè§£è¯»çŠ¶æ€:")
        print(f"  æ‰¹æ¬¡ID: {args.batch_id}")
        print(f"  ç­›é€‰ç±»å‹: {args.filter_type}")
        print(f"  æ€»è®ºæ–‡æ•°: {status['total']} ç¯‡")
        print(f"  å·²ç”ŸæˆAIè§£è¯»: {status['interpreted']} ç¯‡")
        print(f"  æœªç”ŸæˆAIè§£è¯»: {status['remaining']} ç¯‡")
        print(f"  å®Œæˆç‡: {status['interpreted']/status['total']*100:.1f}%")
        return
    
    # åªåŠ è½½æ–‡ä»¶åˆ°æ•°æ®åº“
    if args.only_load:
        logger.info("ä»æ–‡ä»¶åŠ è½½AIè§£è¯»ç»“æœåˆ°æ•°æ®åº“...")
        result = load_interpretations_to_database(args.output_dir)
        logger.success(f"æ•°æ®åº“åŠ è½½å®Œæˆ: æˆåŠŸ {result['success']}, å¤±è´¥ {result['failed']}, è·³è¿‡ {result['skipped']}")
        return
    
    # AIè§£è¯»ç”Ÿæˆé˜¶æ®µ
    with SessionLocal() as session:
        papers = CandidatePoolService.get_candidate_papers(session, args.batch_id, args.filter_type)
        logger.info(f"å€™é€‰æ± åŒ…å« {len(papers)} ç¯‡ {args.filter_type} è®ºæ–‡")
    
    if not papers:
        logger.warning("å€™é€‰æ± ä¸ºç©ºï¼Œè¯·å…ˆåˆ›å»ºå€™é€‰æ± ")
        return
    
    logger.info(f"å¼€å§‹ç”ŸæˆAIè§£è¯»å¹¶ä¿å­˜åˆ°æ–‡ä»¶ ({args.output_dir})...")
    interpret_result = generate_interpretations_to_files(papers, args.output_dir, args.max_workers)
    logger.success(f"AIè§£è¯»ç”Ÿæˆå®Œæˆ: æˆåŠŸ {interpret_result['success']}, å¤±è´¥ {interpret_result['failed']}, è·³è¿‡ {interpret_result['skipped']}")
    
    # è‡ªåŠ¨åŠ è½½åˆ°æ•°æ®åº“ï¼ˆé™¤éæŒ‡å®šåªç”Ÿæˆï¼‰
    if not args.only_interpret:
        logger.info("åŠ è½½AIè§£è¯»ç»“æœåˆ°æ•°æ®åº“...")
        load_result = load_interpretations_to_database(args.output_dir)
        logger.success(f"æ•°æ®åº“åŠ è½½å®Œæˆ: æˆåŠŸ {load_result['success']}, å¤±è´¥ {load_result['failed']}, è·³è¿‡ {load_result['skipped']}")
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        final_status = get_interpretation_status(args.batch_id, args.filter_type)
        print(f"\nğŸ‰ AIè§£è¯»ç”Ÿæˆä»»åŠ¡å®Œæˆ!")
        print(f"  å€™é€‰æ± : {final_status['total']} ç¯‡ {args.filter_type} è®ºæ–‡")
        print(f"  AIè§£è¯»å®Œæˆ: {final_status['interpreted']} ç¯‡ ({final_status['interpreted']/final_status['total']*100:.1f}%)")


if __name__ == "__main__":
    main()
