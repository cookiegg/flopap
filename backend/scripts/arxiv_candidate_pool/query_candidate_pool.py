#!/usr/bin/env python3
"""
æŸ¥è¯¢å€™é€‰è®ºæ–‡æ± ç»Ÿè®¡ä¿¡æ¯

æŸ¥çœ‹æŒ‡å®šæ—¥æœŸçš„å€™é€‰æ± ç»Ÿè®¡å’Œè®ºæ–‡ä¿¡æ¯
"""
import argparse
import os
import sys
from pathlib import Path

# ç¦ç”¨ä»£ç†
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('all_proxy', None)
os.environ.pop('ALL_PROXY', None)

# æ·»åŠ backendæ ¹ç›®å½•åˆ°è·¯å¾„
backend_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_root))

import pendulum
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.db.session import SessionLocal
from app.services.data_ingestion.arxiv_candidate_pool import CandidatePoolServiceV2


def main():
    parser = argparse.ArgumentParser(description="æŸ¥è¯¢å€™é€‰è®ºæ–‡æ± ")
    parser.add_argument(
        "--date",
        type=str,
        help="æŸ¥è¯¢æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)ï¼Œé»˜è®¤ä¸ºä»Šå¤©"
    )
    parser.add_argument(
        "--filter-type",
        choices=['cs', 'ai-ml-cv', 'math', 'physics', 'all'],
        help="æŸ¥è¯¢ç‰¹å®šç­›é€‰ç±»å‹"
    )
    parser.add_argument(
        "--show-papers",
        action="store_true",
        help="æ˜¾ç¤ºè®ºæ–‡è¯¦ç»†ä¿¡æ¯"
    )
    args = parser.parse_args()
    
    # ç¡®å®šæŸ¥è¯¢æ—¥æœŸ
    if args.date:
        target_date = pendulum.parse(args.date).date()
    else:
        target_date = pendulum.today().date()
    
    print(f"=== å€™é€‰è®ºæ–‡æ± æŸ¥è¯¢ - {target_date} ===")
    
    # è·å–æ•°æ®åº“ä¼šè¯
    db = SessionLocal()
    
    try:
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        
        # ç›´æ¥æŸ¥è¯¢å€™é€‰æ± æ•°æ®
        from app.services.data_ingestion.arxiv_candidate_pool import date_to_uuid
        from app.models import CandidatePool
        from sqlalchemy import select, func
        
        date_uuid = date_to_uuid(target_date)
        
        # æŒ‰ç­›é€‰ç±»å‹ç»Ÿè®¡
        stmt = (
            select(CandidatePool.filter_type, func.count(CandidatePool.paper_id))
            .where(CandidatePool.batch_id == date_uuid)
            .group_by(CandidatePool.filter_type)
        )
        
        results = db.execute(stmt).all()
        
        if not results:
            print(f"âŒ æœªæ‰¾åˆ° {target_date} çš„å€™é€‰æ± æ•°æ®")
            return
        
        total = 0
        stats = {}
        for filter_type, count in results:
            print(f"  {filter_type.upper()}: {count} ç¯‡")
            stats[filter_type] = count
            total += count
        print(f"  æ€»è®¡: {total} ç¯‡")
        
        # å¦‚æœæŒ‡å®šäº†ç­›é€‰ç±»å‹ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if args.filter_type:
            paper_ids = CandidatePoolServiceV2.get_candidate_papers_by_date(
                db, target_date, args.filter_type
            )
            
            print(f"\nğŸ” {args.filter_type.upper()}å€™é€‰æ± è¯¦æƒ…:")
            print(f"è®ºæ–‡æ•°é‡: {len(paper_ids)}")
            
            if args.show_papers and paper_ids:
                print(f"\nğŸ“„ è®ºæ–‡åˆ—è¡¨:")
                # è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯
                result = db.execute(text("""
                    SELECT p.arxiv_id, p.title, p.categories
                    FROM papers p
                    WHERE p.id = ANY(:paper_ids)
                    ORDER BY p.submitted_date DESC
                    LIMIT 10
                """), {"paper_ids": paper_ids[:10]})
                
                papers = result.fetchall()
                for i, paper in enumerate(papers, 1):
                    arxiv_id, title, categories = paper
                    print(f"  {i}. {arxiv_id}")
                    print(f"     æ ‡é¢˜: {title[:80]}...")
                    print(f"     åˆ†ç±»: {', '.join(categories)}")
                    print()
                
                if len(paper_ids) > 10:
                    print(f"  ... è¿˜æœ‰ {len(paper_ids) - 10} ç¯‡è®ºæ–‡")
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        raise
    finally:
        db.close()


if __name__ == "__main__":
    main()
