{
  "paper_id": "6d189f75-cb42-4630-b26a-fb92185a9d13",
  "arxiv_id": "2512.12387v1",
  "title_en": "Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment",
  "title_zh": "基于时间与群体维度价值锚定的流匹配模型对齐方法",
  "summary_en": "Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.",
  "summary_zh": "群体相对策略优化（GRPO）已被证明在增强大语言模型（LLMs）的对齐能力方面极为有效。然而，当前将GRPO应用于基于流匹配的图像生成时，忽视了其核心原则与视觉合成过程独特动态之间的根本性冲突。这种不匹配导致两个关键局限：（一）在所有时间步上统一施加稀疏的终端奖励会损害时间信用分配，忽略了从早期结构形成到后期细调等不同生成阶段的关键性差异；（二）仅依赖相对的组内奖励会导致优化信号随训练收敛而衰减，当奖励多样性完全耗尽时引发优化停滞。为解决这些局限，我们提出了价值锚定群体策略优化（VGPO），该框架在时间与群体维度上重新定义了价值估计。具体而言，VGPO将稀疏的终端奖励转化为密集的、过程感知的价值估计，通过对每个生成阶段的预期累积奖励进行建模，实现精确的信用分配。此外，VGPO以经过绝对值增强的新型过程替代标准的群体归一化，从而在奖励多样性下降时仍能维持稳定的优化信号。在三个基准数据集上的大量实验表明，VGPO在实现最先进图像质量的同时，提升了任务特定准确性，并有效缓解了奖励破解问题。项目网页：https://yawen-shao.github.io/VGPO/。",
  "timestamp": "2025-12-16T22:24:38.843241",
  "model_name": "deepseek-reasoner"
}