"""
ä¼šè®®è®ºæ–‡å¯¼å…¥æœåŠ¡ - 2025å¹´ä¼šè®®æ•°æ®å¯¼å…¥

æ”¯æŒä» data/paperlists ç›®å½•å¯¼å…¥2025å¹´ä¼šè®®è®ºæ–‡æ•°æ®
"""

import json
import uuid
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

import pendulum
from sqlalchemy.orm import Session
from sqlalchemy import select

from app.core.config import settings
from app.models import Paper, DataSource, IngestionBatch
from app.models.data_source import DataSourceType


# 2025å¹´æ”¯æŒçš„ä¼šè®®åˆ—è¡¨
SUPPORTED_2025_CONFERENCES = {
    'neurips2025': {'name': 'NeurIPS 2025', 'category_prefix': 'neurips'},
    'iclr2025': {'name': 'ICLR 2025', 'category_prefix': 'iclr'},
    'icml2025': {'name': 'ICML 2025', 'category_prefix': 'icml'},
    'cvpr2025': {'name': 'CVPR 2025', 'category_prefix': 'cvpr'},
    'iccv2025': {'name': 'ICCV 2025', 'category_prefix': 'iccv'},
    'aaai2025': {'name': 'AAAI 2025', 'category_prefix': 'aaai'},
    'acl2025': {'name': 'ACL 2025', 'category_prefix': 'acl'},
    'naacl2025': {'name': 'NAACL 2025', 'category_prefix': 'naacl'},
    'coling2025': {'name': 'COLING 2025', 'category_prefix': 'coling'},
    'aistats2025': {'name': 'AISTATS 2025', 'category_prefix': 'aistats'},
    'wacv2025': {'name': 'WACV 2025', 'category_prefix': 'wacv'},
    'www2025': {'name': 'WWW 2025', 'category_prefix': 'www'},
    'corl2025': {'name': 'CoRL 2025', 'category_prefix': 'corl'},
    'colm2025': {'name': 'COLM 2025', 'category_prefix': 'colm'},
    'siggraph2025': {'name': 'SIGGRAPH 2025', 'category_prefix': 'siggraph'},
    'rss2025': {'name': 'RSS 2025', 'category_prefix': 'rss'},
    '3dv2025': {'name': '3DV 2025', 'category_prefix': '3dv'},
    'alt2025': {'name': 'ALT 2025', 'category_prefix': 'alt'},
    'ai4x2025': {'name': 'AI4X 2025', 'category_prefix': 'ai4x'},
}


def get_conference_data_path(conference_id: str) -> Path:
    """è·å–ä¼šè®®æ•°æ®æ–‡ä»¶è·¯å¾„"""
    # ä» conference_id æå–ä¼šè®®åç§° (å¦‚ neurips2025 -> nips)
    conf_name = conference_id.replace('2025', '')
    if conf_name == 'neurips':
        conf_name = 'nips'  # ç‰¹æ®Šå¤„ç†
    
    data_dir = settings.project_root / 'data' / 'paperlists' / conf_name
    return data_dir / f'{conference_id}.json'


def convert_conference_paper(paper_data: Dict[str, Any], conference_id: str) -> Dict[str, Any]:
    """è½¬æ¢ä¼šè®®è®ºæ–‡æ•°æ®æ ¼å¼"""
    conf_info = SUPPORTED_2025_CONFERENCES[conference_id]
    
    # ç”Ÿæˆ arxiv_id (ä¼šè®®è®ºæ–‡ä½¿ç”¨ä¼šè®®IDæ ¼å¼)
    arxiv_id = f"{conference_id}.{paper_data.get('id', str(uuid.uuid4())[:8])}"
    
    # å¤„ç†ä½œè€…ä¿¡æ¯
    authors_str = paper_data.get('author', '')
    if authors_str:
        author_names = [name.strip() for name in authors_str.split(';') if name.strip()]
        authors = [{'name': name} for name in author_names]
    else:
        authors = [{'name': 'Unknown'}]
    
    # å¤„ç†åˆ†ç±»ä¿¡æ¯
    primary_area = paper_data.get('primary_area', 'general')
    categories = [f"{conf_info['category_prefix']}.{primary_area}"]
    
    # å¤„ç†æ—¥æœŸ (2025å¹´ä¼šè®®ç»Ÿä¸€ä½¿ç”¨2025å¹´æ—¥æœŸ)
    submitted_date = pendulum.parse('2025-01-01T00:00:00Z')
    
    # å¤„ç†PDFé“¾æ¥
    pdf_url = None
    if 'site' in paper_data and paper_data['site']:
        pdf_url = paper_data['site']
    
    return {
        'arxiv_id': arxiv_id,
        'title': paper_data.get('title', 'Untitled'),
        'summary': paper_data.get('abstract', ''),
        'authors': authors,
        'categories': categories,
        'submitted_date': submitted_date,
        'updated_date': None,
        'pdf_url': pdf_url,
        'html_url': pdf_url,
        'comment': paper_data.get('tldr', ''),
        'doi': None,
        'primary_category': f"{conf_info['category_prefix']}.{primary_area}",
        'source': f'conf/{conference_id}',  # ä¿®æ”¹ï¼šä½¿ç”¨ conf/ å‰ç¼€
    }


def import_conference_papers(session: Session, conference_id: str) -> IngestionBatch:
    """å¯¼å…¥æŒ‡å®šä¼šè®®çš„è®ºæ–‡æ•°æ®"""
    
    if conference_id not in SUPPORTED_2025_CONFERENCES:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼šè®®: {conference_id}")
    
    conf_info = SUPPORTED_2025_CONFERENCES[conference_id]
    data_path = get_conference_data_path(conference_id)
    
    if not data_path.exists():
        raise FileNotFoundError(f"ä¼šè®®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    print(f"ğŸ“š å¼€å§‹å¯¼å…¥ {conf_info['name']} è®ºæ–‡æ•°æ®...")
    
    # åˆ›å»ºæˆ–è·å–æ•°æ®æºé…ç½®
    data_source = session.scalar(select(DataSource).where(DataSource.prefix == conference_id))
    if not data_source:
        data_source = DataSource(
            prefix=conference_id,
            name=conf_info['name'],
            source_type=DataSourceType.STATIC,
            is_active=True
        )
        session.add(data_source)
        session.flush()
    
    # åˆ›å»ºæ‘„å–æ‰¹æ¬¡
    batch = IngestionBatch(
        source_date=pendulum.now().date(),
        fetched_at=pendulum.now(),
        item_count=0,
        query=f"conference:{conference_id}",
        notes=f"Import {conf_info['name']} papers from JSON file"
    )
    session.add(batch)
    session.flush()
    
    # è¯»å–JSONæ•°æ®
    with open(data_path, 'r', encoding='utf-8') as f:
        papers_data = json.load(f)
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(papers_data)} ç¯‡è®ºæ–‡")
    
    # å¯¼å…¥è®ºæ–‡æ•°æ®
    imported_count = 0
    skipped_count = 0
    
    for paper_data in papers_data:
        try:
            converted_data = convert_conference_paper(paper_data, conference_id)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = session.scalar(
                select(Paper).where(Paper.arxiv_id == converted_data['arxiv_id'])
            )
            
            if existing is None:
                paper = Paper(
                    arxiv_id=converted_data['arxiv_id'],
                    title=converted_data['title'],
                    summary=converted_data['summary'],
                    authors=converted_data['authors'],
                    categories=converted_data['categories'],
                    submitted_date=converted_data['submitted_date'],
                    updated_date=converted_data['updated_date'],
                    pdf_url=converted_data['pdf_url'],
                    html_url=converted_data['html_url'],
                    comment=converted_data['comment'],
                    doi=converted_data['doi'],
                    primary_category=converted_data['primary_category'],
                    source=converted_data['source'],
                    ingestion_batch_id=batch.id,
                )
                session.add(paper)
                imported_count += 1
            else:
                skipped_count += 1
                
        except Exception as e:
            print(f"âš ï¸  è·³è¿‡è®ºæ–‡ {paper_data.get('id', 'unknown')}: {e}")
            skipped_count += 1
    
    # æ›´æ–°æ‰¹æ¬¡ä¿¡æ¯
    batch.item_count = imported_count
    session.commit()
    
    print(f"âœ… å¯¼å…¥å®Œæˆ: {imported_count} ç¯‡æ–°è®ºæ–‡, {skipped_count} ç¯‡è·³è¿‡")
    return batch


def import_all_2025_conferences(session: Session) -> Dict[str, IngestionBatch]:
    """å¯¼å…¥æ‰€æœ‰2025å¹´ä¼šè®®æ•°æ®"""
    results = {}
    
    print("ğŸ›ï¸ å¼€å§‹å¯¼å…¥æ‰€æœ‰2025å¹´ä¼šè®®æ•°æ®...")
    
    for conference_id in SUPPORTED_2025_CONFERENCES:
        try:
            batch = import_conference_papers(session, conference_id)
            results[conference_id] = batch
            print(f"âœ… {conference_id}: {batch.item_count} ç¯‡è®ºæ–‡")
        except Exception as e:
            print(f"âŒ {conference_id}: {e}")
            results[conference_id] = None
    
    return results


def get_available_2025_conferences() -> List[Dict[str, str]]:
    """è·å–å¯ç”¨çš„2025å¹´ä¼šè®®åˆ—è¡¨"""
    available = []
    
    for conference_id, conf_info in SUPPORTED_2025_CONFERENCES.items():
        data_path = get_conference_data_path(conference_id)
        if data_path.exists():
            available.append({
                'id': conference_id,
                'name': conf_info['name'],
                'data_path': str(data_path),
                'file_size': data_path.stat().st_size
            })
    
    return available
