{
  "paper_id": "46015e4c-c458-4440-9676-31085eeb51df",
  "arxiv_id": "2512.12448v1",
  "title_en": "Optimized Architectures for Kolmogorov-Arnold Networks",
  "title_zh": "Kolmogorov-Arnold 网络的优化架构",
  "summary_en": "Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.",
  "summary_zh": "通过架构改进来提升 Kolmogorov-Arnold 网络（KANs）性能的努力，常因其引入的复杂性而受阻，这损害了 KANs 最初吸引人的可解释性。本研究采用过度配置的架构结合稀疏化技术，以在不牺牲准确性的前提下学习紧凑、可解释的 KANs。关键之处在于，我们专注于可微稀疏化方法，从而将架构搜索转化为端到端的优化问题。在函数逼近基准测试、动力系统预测以及实际预测任务中，我们证明了该方法在发现显著更小模型的同时，能够达到具有竞争力或更优的准确性。过度配置与稀疏化具有协同效应，二者结合的表现优于单独使用任一方法。这为获得兼具更强表达能力和更高可解释性的模型提供了一条原则性路径，从而应对科学机器学习中的一个核心矛盾。",
  "timestamp": "2025-12-16T22:25:25.596793",
  "model_name": "deepseek-reasoner"
}