{
  "paper_id": "155393a0-488c-4dcd-ab5e-c7bf427f5bbe",
  "arxiv_id": "2512.12367v1",
  "title_en": "JPEG-Inspired Cloud-Edge Holography",
  "title_zh": "受JPEG启发的云边全息术",
  "summary_en": "Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $<$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.",
  "summary_zh": "计算机生成全息术为增强现实和虚拟现实中的近眼显示提供了一种变革性解决方案。深度学习的最新进展极大地提升了全息图的重建质量与计算效率。然而，在紧凑的眼镜式设备上直接部署神经全息计算流程，受到计算与能耗的严格限制；而采用云端卸载配合自然图像编解码器传输的方案，往往会扭曲相位信息，且需要高带宽以维持重建质量。神经压缩方法虽能降低带宽，但需在边缘设备部署沉重的神经解码器，从而增加推理延迟与硬件需求。本研究提出一种受JPEG启发的云边全息术，该高效流程围绕可学习的变换编解码器设计，保留了JPEG的块结构特性与硬件友好性。我们的系统将所有繁重的神经处理移至云端，边缘设备仅执行轻量级解码而无需任何神经推理。为进一步提升吞吐量，我们在云端与边缘均实现了用于熵编码的定制CUDA内核。该设计在每像素＜2比特的码率下实现了32.15 dB的峰值信噪比，解码延迟低至4.2毫秒。数值模拟与光学实验均证实了全息图的高重建质量。通过将全息计算与既保持JPEG结构效率、又通过可学习组件进行扩展的编解码器相结合，本框架能够在资源受限的可穿戴设备上实现低延迟、高带宽效率的全息图流式传输——仅需现代片上系统即可支持的简易块状解码，无需神经解码器或专用硬件。",
  "timestamp": "2025-12-16T22:25:30.647191",
  "model_name": "deepseek-reasoner"
}