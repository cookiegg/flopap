{
  "paper_id": "10bdacaf-a3c4-4dab-b6c1-c26a41df4b05",
  "arxiv_id": "2512.12461v1",
  "title_en": "Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling",
  "title_zh": "跨模态表征知识蒸馏增强脉冲信息局部场电位建模",
  "summary_en": "Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.",
  "summary_zh": "在皮层内神经实验中，局部场电位（LFPs）常与脉冲活动同步记录，能够测量更大互补时空尺度的大脑活动以供科学研究，且相较于脉冲信号具有诸多实用优势，包括更高的长期稳定性、对电极退化的更强鲁棒性以及更低的功耗需求。尽管存在这些优势，近期的神经建模框架仍主要聚焦于脉冲活动，因为LFP信号由于其聚合性、群体水平的特性存在固有的建模挑战，常导致对下游任务变量（如运动行为）的预测能力较低。为应对这一挑战，我们提出一种跨模态知识蒸馏框架，将预训练的多会话脉冲Transformer模型中的高保真表征知识迁移至LFP Transformer模型。具体而言，我们首先采用面向特定会话的神经标记化策略，通过掩码自编码目标跨多个记录会话训练教师脉冲模型。随后，我们将学生LFP模型的潜在表征与教师脉冲模型的对齐。实验结果表明，蒸馏后的LFP模型在完全无监督和有监督设定下均持续优于单会话及多会话LFP基线模型，且无需额外蒸馏即可泛化至其他会话，同时保持卓越性能。这些发现证明，跨模态知识蒸馏是一种强大且可扩展的方法，能够利用高性能脉冲模型开发更精确的LFP模型。",
  "timestamp": "2025-12-16T22:25:32.767538",
  "model_name": "deepseek-reasoner"
}