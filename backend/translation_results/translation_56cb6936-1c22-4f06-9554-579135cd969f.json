{
  "paper_id": "56cb6936-1c22-4f06-9554-579135cd969f",
  "arxiv_id": "2512.12284v1",
  "title_en": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval",
  "title_zh": "V-Rex：基于动态KV缓存检索的实时流式视频大语言模型加速方法",
  "summary_en": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.",
  "summary_zh": "流式视频大语言模型正日益广泛应用于视频描述生成、问答系统、对话代理和增强现实等实时多模态任务。然而，由于持续输入的流式视频数据会导致键值缓存随视频帧显著增长，这类模型面临着根本性的内存与计算挑战。该过程需要迭代式的预填充阶段，这是流式视频大语言模型的独有特征。正是由于这种迭代预填充机制，模型存在显著局限性，包括计算开销巨大、数据传输量庞大以及准确率下降等问题。值得注意的是，在以边缘部署为主要目标的应用场景中，这一问题尤为突出。",
  "timestamp": "2025-12-16T22:25:51.662920",
  "model_name": "deepseek-reasoner"
}