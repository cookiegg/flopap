{
  "paper_id": "6d189f75-cb42-4630-b26a-fb92185a9d13",
  "arxiv_id": "2512.12387v1",
  "title_en": "Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment",
  "title_zh": "基于时间与群体维度价值锚定的流匹配模型对齐方法",
  "summary_en": "Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.",
  "summary_zh": "群体相对策略优化（GRPO）已被证明在增强大语言模型的对齐能力方面极为有效。然而，当前将GRPO应用于基于流匹配的图像生成时，忽视了其核心原则与视觉合成过程独特动态之间的根本性冲突。这种不匹配导致两个关键局限：（一）在所有时间步上统一施加稀疏的终端奖励会损害时间信用分配，忽略了从早期结构形成到后期调优等不同生成阶段的关键性差异。（二）仅依赖相对的组内奖励会导致优化信号随训练收敛而衰减，当奖励多样性完全耗尽时引发优化停滞。为解决这些局限，我们提出价值锚定群体策略优化（VGPO），这是一个在时间和群体维度上重新定义价值估计的框架。具体而言，VGPO将稀疏的终端奖励转化为密集的、过程感知的价值估计，通过对每个生成阶段的预期累积奖励进行建模，实现精确的信用分配。此外，VGPO采用一种通过绝对值增强的新颖过程替代标准群体归一化，以在奖励多样性下降时仍保持稳定的优化信号。在三个基准测试上的大量实验表明，VGPO在实现最先进图像质量的同时，提升了任务特定准确性，并有效缓解了奖励破解问题。项目网页：https://yawen-shao.github.io/VGPO/。",
  "timestamp": "2025-12-16T22:25:28.394660",
  "model_name": "deepseek-reasoner"
}