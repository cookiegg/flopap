{
  "paper_id": "3a0e02a6-7476-429b-b936-e2ca855ade51",
  "arxiv_id": "2512.12411v1",
  "title_en": "Feeling the Strength but Not the Source: Partial Introspection in LLMs",
  "title_zh": "感知强度而非来源：大语言模型中的部分内省能力",
  "summary_en": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.",
  "summary_zh": "Anthropic 近期研究声称，前沿模型有时能够检测并命名以激活方向形式注入的“概念”。我们对这一论断的稳健性进行了检验。首先，我们在 Meta-Llama-3.1-8B-Instruct 模型上复现了 Anthropic 的多轮“涌现内省”结果，发现该模型在 Anthropic 原始流程下识别并命名注入概念的成功率为 20%，与其报告数据完全一致，这表明内省能力并非超大规模或高性能模型所独有。其次，我们系统性地调整了推理提示，发现内省能力具有脆弱性：在密切相关的任务（如对注入概念的多选识别，或对是否注入概念进行二元判别的不同提示）上，其性能显著下降。第三，我们发现了一种与之形成对比的部分内省机制：同一模型能够以高达 70% 的准确率（远超 25% 的随机基线）可靠地对归一化注入概念向量的系数强度（弱/中/强/极强）进行分类。这些结果共同为 Anthropic 的论断提供了更多证据，即语言模型在内省过程中有效地计算了其基线内部表征的函数；然而，模型关于这些表征的自我报告是狭窄且对提示敏感的。我们的代码公开于：https://github.com/elyhahami18/CS2881-Introspection。",
  "timestamp": "2025-12-16T22:25:38.605016",
  "model_name": "deepseek-reasoner"
}