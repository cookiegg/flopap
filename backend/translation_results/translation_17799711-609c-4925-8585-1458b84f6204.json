{
  "paper_id": "17799711-609c-4925-8585-1458b84f6204",
  "arxiv_id": "2512.12281v1",
  "title_en": "Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection",
  "title_zh": "Cognitive-YOLO：基于数据第一性原理、由大语言模型驱动的目标检测架构合成方法",
  "summary_en": "Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data \"first principles\" is more critical for achieving a superior architecture than simply retrieving SOTA components.",
  "summary_zh": "设计高性能的目标检测架构是一项复杂任务，传统人工设计方式耗时费力，而神经架构搜索（NAS）则计算成本高昂。尽管近期利用大语言模型（LLM）的方法展现出潜力，但它们通常仅在搜索循环中充当迭代优化器，而非基于对数据的整体理解直接生成架构。为弥补这一不足，我们提出Cognitive-YOLO——一种由大语言模型驱动的架构合成新框架，能够直接从数据集的内在特征生成网络配置。该方法包含三个阶段：首先，分析模块从目标数据集中提取关键元特征（如目标尺度分布、场景密度等）；其次，大语言模型基于这些特征进行推理，并结合通过检索增强生成（RAG）技术获取的先进组件知识，将架构合成为结构化的神经架构描述语言（NADL）；最后，编译器将该描述实例化为可部署模型。在五个多样化的目标检测数据集上的大量实验表明，我们提出的Cognitive-YOLO能够持续生成更优的架构，在多个基准测试中取得了极具竞争力的性能，并展现出优于强基线模型的参数-性能权衡比。关键的是，我们的消融实验证明，大语言模型基于数据的推理是性能提升的主要驱动力，这表明深刻理解数据的“第一性原理”对于获得优越架构而言，比单纯检索先进组件更为关键。",
  "timestamp": "2025-12-16T22:25:48.187591",
  "model_name": "deepseek-reasoner"
}