{
  "paper_id": "7d0e7054-35eb-49bb-a39f-be2c577d4a2f",
  "arxiv_id": "2512.12465v1",
  "title_en": "Exploring the Design Space of Transition Matching",
  "title_zh": "探索过渡匹配的设计空间",
  "summary_en": "Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller \"head\" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.",
  "summary_zh": "过渡匹配（TM）是一种新兴的生成建模范式，它推广了扩散模型、流匹配模型以及连续状态自回归模型。与先前范式类似，TM 逐渐将噪声样本转换为数据样本，但其采用了一个第二层“内部”生成模型来实现过渡步骤，使得过渡过程相比扩散模型和流模型更具表达力。为使该范式易于处理，TM 使用一个大型骨干网络和一个较小的“头部”模块来高效执行生成过渡步骤。本研究对 TM 框架（重点关注其时间连续双向变体）中头部模块的设计、训练和采样进行了大规模系统性探究。通过全面的消融实验和训练 56 个不同的 17 亿参数文生图模型（共产生 549 次独立评估），我们评估了头部模块架构和训练建模方式的影响，并探索了一类实用的随机 TM 采样器。我们分析了其对生成质量、训练效率和推理效率的影响。研究发现，采用 MLP 头部、配合特定时间加权训练策略及高频采样器的 TM 在所有评估指标中综合排名最佳，达到了所测试基线中的最先进水平；而采用 Transformer 头部、序列缩放及低频采样的方案位列其次，在图像美学表现上尤为突出。最后，我们认为本文实验揭示了最可能提升质量和效率的设计要素，同时也指出了哪些设计选择不太可能带来进一步增益。",
  "timestamp": "2025-12-16T22:25:28.681788",
  "model_name": "deepseek-reasoner"
}