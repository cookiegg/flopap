{
  "paper_id": "37acc913-6e2c-43ba-aae2-e20d32cf1fe8",
  "arxiv_id": "2512.12107v1",
  "title_en": "EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography",
  "title_zh": "EchoVLM：基于测量基础的多模态超声心动图学习",
  "summary_en": "Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.",
  "summary_zh": "超声心动图是心脏病学中应用最广泛的成像技术，但其解读过程仍高度依赖人工且本质上是多模态的，需要视图识别、定量测量、定性评估以及基于指南的推理。尽管近期视觉语言模型在自然图像和某些医学领域取得了广泛成功，但其在超声心动图领域的应用潜力一直受到限制，原因在于缺乏大规模、临床基础扎实的图文数据集，以及缺少超声心动图解读核心所需的基于测量的推理能力。我们提出了首个基于测量基础的多模态超声心动图数据集EchoGround-MIMIC，该数据集包含来自1,572名患者的19,065个图文对，涵盖标准化视图、结构化测量、基于测量的描述文本以及依据临床指南生成的疾病标签。基于此资源，我们提出了视觉语言模型EchoVLM，该模型引入了两项新颖的预训练目标：（一）视图感知对比损失，用于编码超声心动图成像中视图依赖的结构特征；（二）否定感知对比损失，用于区分临床关键性的阴性与阳性发现。在涵盖多模态疾病分类、图文检索、视图分类、心腔分割和关键点检测等五大类共36项临床任务的评估中，EchoVLM均取得了最先进的性能（零样本疾病分类AUC达86.5%，视图分类准确率达95.1%）。我们证明了基于临床实践的多模态预训练能够产生可迁移的视觉表征，并将EchoVLM确立为端到端超声心动图解读的基础模型。我们将公开EchoGround-MIMIC数据集及数据整理代码，以促进多模态超声心动图解读研究的可复现性与进一步探索。",
  "timestamp": "2025-12-16T22:26:06.131766",
  "model_name": "deepseek-reasoner"
}