{
  "paper_id": "762bb1bf-b51e-4d5f-ba30-1dee4ace40b0",
  "arxiv_id": "2512.12246v1",
  "title_en": "Moment and Highlight Detection via MLLM Frame Segmentation",
  "title_zh": "基于多模态大语言模型帧分割的时刻与高光片段检测",
  "summary_en": "Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous \"0\" and/or \"1\" characters, with one character per frame. The \"0\"/\"1\" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.",
  "summary_zh": "基于Transformer的方法已实现对自然语言查询的视频时刻与高光片段的统一检测。另有研究利用生成式多模态大语言模型（MLLM）的推理能力，通过生成文本时间戳来预测时刻及/或高光片段。此类方法虽有效，但基于文本的生成无法为帧级预测提供直接梯度，因为模型仅输出语言标记。尽管近期强化学习方法尝试解决该问题，我们提出一种创新方案：直接在LLM的输出标记上应用分割目标函数。模型接收固定数量的视频帧及提示指令，该指令强制模型输出连续的“0”和/或“1”字符序列，每个字符对应一帧。字符“0”和“1”既受益于LLM固有的语言能力，又分别作为背景与前景概率。训练过程结合分割损失函数与常规因果语言模型损失函数进行优化。推理阶段通过束搜索生成序列及其逻辑值，分别作为时刻预测结果与显著性分数。尽管仅采样25帧（不足同类方法采样量的一半），本方法在QVHighlights数据集上实现了强劲的高光检测性能（HIT@1达56.74）。同时，在时刻检索任务中，本高效方法的平均精度（35.28 MAP）超越基线水平。实验表明，即使因果语言模型损失进入平台期，分割损失仍能提供稳定的互补学习信号。",
  "timestamp": "2025-12-16T22:25:52.149092",
  "model_name": "deepseek-reasoner"
}